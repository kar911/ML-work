{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "from PIL import Image\n",
    "\"SkalskiP/better-florence-2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7899\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 580, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1938, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1761, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/components/chatbot.py\", line 315, in postprocess\n",
      "    self._postprocess_chat_messages(message_pair[1]),\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/components/chatbot.py\", line 276, in _postprocess_chat_messages\n",
      "    return create_file_message(chat_message, filepath)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/components/chatbot.py\", line 250, in create_file_message\n",
      "    return FileMessage(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/pydantic/main.py\", line 176, in __init__\n",
      "    self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for FileMessage\n",
      "alt_text\n",
      "  Input should be a valid string [type=string_type, input_value=[None, \"\\nOnce upon a tim... changed his life.</s>\"], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/string_type\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        \n",
    "        # msg = gr.Textbox()\n",
    "        clear = gr.Button(\"Clear\")\n",
    "        t=gr.Text(\"\")\n",
    "        image=gr.Image(type='pil')\n",
    "        chatbot = gr.Chatbot()\n",
    "\n",
    "        text=[]\n",
    "        \n",
    "        def user(user_message, history):\n",
    "            return \"\", history + [[user_message, None]]\n",
    "\n",
    "        async def bot(history):\n",
    "            bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
    "            # history= \"\"\n",
    "            global text\n",
    "            # print()\n",
    "            async for character in chain.astream(history):\n",
    "                text.append([None,character])\n",
    "                \n",
    "        def bot2():\n",
    "            global text\n",
    "            # print()\n",
    "            for character in text:\n",
    "                text.append([None,character])\n",
    "                time.sleep(0.05)\n",
    "                yield text\n",
    "            # async for character in chain.astream(history):\n",
    "            #     text.append([None,character])\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "        image.upload(bot, image, chatbot,queue=False).then(bot2, None, chatbot)\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "        # chatbot.change(lambda _ : chatbot.value([[\"\",\"helllo\"]]))\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kar911/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from PIL import Image\n",
    "\n",
    "pipe1 = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "pipe2 = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "HUGGINGFACEHUB_API_TOKEN='hf_fytqFOBCexHEJOGMFpFHAiUrvnWPaeRU'\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    # max_length=100,\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")\n",
    "def reduce_add(a):\n",
    "    ll=dict()\n",
    "    for i in a:\n",
    "        if i['score'] > 0.89:\n",
    "            if i['label'] not in ll.keys():\n",
    "                ll[i['label']] = 1\n",
    "            else:\n",
    "                ll[i['label']]+=1\n",
    "    return \"there are \\n\"+', \\n'.join([str(i[1])+' '+i[0]  for i  in ll.items() ])\n",
    "\n",
    "def image_segmentation_tool(image):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe1(image)\n",
    "    if reduce_add(segmentation_results) == \"there are \\n\":\n",
    "        raise Passs()\n",
    "    return reduce_add(segmentation_results)\n",
    "\n",
    "def image_caption_tool(image):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe2(image)\n",
    "    if segmentation_results[0][\"generated_text\"] == \"\":\n",
    "        raise Passs(\"no result found use different image to create story\") \n",
    "    return segmentation_results[0][\"generated_text\"]\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def story_generation_tool(segmentation_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a storyteller. Based on the following segmentation results, create a story:\n",
    "    {segmentation_results}\n",
    "    Story:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    story = prompt | llm\n",
    "    return story.invoke(input={\"segmentation_results\":segmentation_results})\n",
    "\n",
    "# def translation_tool(english_text):\n",
    "#     prompt_template = \"\"\"\n",
    "#     You are a translator. Translate the following English text to Hindi:\n",
    "#     {english_text}\n",
    "\n",
    "#     Translation:\n",
    "#     \"\"\"\n",
    "#     prompt = PromptTemplate.from_template(prompt_template)\n",
    "#     translation = prompt | llm\n",
    "#     return translation.invoke(input={\"english_text\": english_text})\n",
    "\n",
    "\n",
    "runnable = RunnableLambda(image_segmentation_tool).with_fallbacks([RunnableLambda(image_caption_tool)])\n",
    "runnable2 = RunnableLambda(story_generation_tool)\n",
    "# runnable3 = RunnableLambda(translation_tool)\n",
    "\n",
    "chain = runnable | runnable2\n",
    "\n",
    "# import gradio as gr\n",
    "\n",
    "# title = \"Image to short Story Generator\"\n",
    "# description = \"\"\"\n",
    "# Upload an image, and this app will generate a short story based on the image.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# async def sepia(input_img):\n",
    "#     chunks=[]\n",
    "#     async for e in chain.astream(input_img):\n",
    "#         print('dd')\n",
    "#         # yield \"\".join(chunks)\n",
    "#         chunks.append(e)\n",
    "#         yield chunks\n",
    "#         # yield \"\".join(chunks)\n",
    "\n",
    "# demo = gr.Interface(sepia, gr.Image(type='pil'), \"textarea\",title=title,\n",
    "#     description=description,live=True\n",
    "# )\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatInterface.__init__() got an unexpected keyword argument 'height'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mBlocks() \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mRow():\n\u001b[0;32m----> 5\u001b[0m         chat\u001b[38;5;241m=\u001b[39m\u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         image\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mImage(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m         clear\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mClearButton([chat,image])\n",
      "\u001b[0;31mTypeError\u001b[0m: ChatInterface.__init__() got an unexpected keyword argument 'height'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        chat=gr.ChatInterface()\n",
    "        image=gr.Image(type='pil')\n",
    "        clear=gr.ClearButton([chat,image])\n",
    "\n",
    "        image.upload(sepia,inputs=image,outputs=chat,queue=False)\n",
    "\n",
    "        def user(user_message, history):\n",
    "            return \"\", history + [[user_message, None]]\n",
    "\n",
    "        def bot(history):\n",
    "            history[-1][1] = \"\"\n",
    "            for character in bot_message:\n",
    "                history[-1][1] += character\n",
    "                time.sleep(0.05)\n",
    "                yield history\n",
    "\n",
    "        image.submit(user, [chat], [chat], queue=False).then(\n",
    "            bot, chat, chat\n",
    "        )\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"togethercomputer/RedPajama-INCITE-Chat-3B-v1\", torch_dtype=torch.float16)\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [29, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def predict(message, history):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    messages = \"\".join([\"\".join([\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]])\n",
    "                for item in history_transformer_format])\n",
    "\n",
    "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=1000,\n",
    "        temperature=1.0,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop])\n",
    "        )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token != '<':\n",
    "            partial_message += new_token\n",
    "            yield partial_message\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALLMODELS(Runnable,object):\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = object.__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        if not hasattr(self, 'initialized'):  # Ensure models are initialized only once\n",
    "            self.object_detection = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "            self.initialized = True\n",
    "\n",
    "    def invoke(self,text):\n",
    "        return \"Invoke method called\"+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7917\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7917/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/routes.py\", line 737, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1938, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1761, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/components/chatbot.py\", line 305, in postprocess\n",
      "    raise TypeError(\n",
      "TypeError: Expected a list of lists or list of tuples. Received: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 541, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1526, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 657, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 650, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 633, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 816, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "  File \"/tmp/ipykernel_4188/1095931847.py\", line 61, in genrrate\n",
      "    chatbot[-1][1]=\"\"\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# # Initialize a ChatInterface\n",
    "# def echo(message, history):\n",
    "#     return message\n",
    "\n",
    "# demo = gr.ChatInterface(fn=echo, examples=[\"hello\", \"hola\", \"merhaba\"], title=\"Echo Bot\")\n",
    "\n",
    "# # Extract the chat element\n",
    "# def get_chat_element():\n",
    "#     return demo.get()\n",
    "\n",
    "# # Use the chat element as the output for an image input\n",
    "# image = gr.Image(type=\"filepath\")\n",
    "# image.upload(get_chat_element, image, demo)\n",
    "\n",
    "# demo.launch()\n",
    "\n",
    "\n",
    "def echo(message, history, system_prompt, tokens):\n",
    "    response = f\"System prompt: {system_prompt}\\n Message: {message}.\"\n",
    "    for i in range(min(len(response), int(tokens))):\n",
    "        time.sleep(0.05)\n",
    "        yield response[: i+1]\n",
    "        \n",
    "import gradio as gr\n",
    "\n",
    "# Initialize a Chatbot\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        image = gr.Image(type=\"filepath\")\n",
    "        t=gr.Text()\n",
    "        chatbot = gr.Chatbot()\n",
    "        \n",
    "\n",
    "        def get_chat_element(g):\n",
    "            return chatbot\n",
    "        \n",
    "        def new(image,chatbot):\n",
    "            full=[]\n",
    "            for i in [(\"\",\"How are you?\"), (\"\",\"I love you\"), (\"\",\"Im very hungry\")]:\n",
    "                print(\"fdfd\")\n",
    "                time.sleep(1)\n",
    "                full.append(i)\n",
    "                chatbot.append(i)\n",
    "                print(chatbot)\n",
    "                yield full\n",
    "\n",
    "        def respond(message, chat_history):\n",
    "            bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
    "            chat_history.append(bot_message)\n",
    "            time.sleep(2)\n",
    "            return  chat_history\n",
    "        \n",
    "        def new1(image,chatbot):\n",
    "            chatbot+=[[\"\",None]]\n",
    "            return \"\",chatbot\n",
    "\n",
    "        def genrrate(chatbot):\n",
    "            bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
    "            chatbot[-1][1]=\"\"\n",
    "            for bm in bot_message:\n",
    "                chatbot[-1][1] +=bm\n",
    "                yield chatbot\n",
    "\n",
    "        image.upload(new1, [image,chatbot], chatbot,queue=False).then(genrrate,chatbot,chatbot)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7881\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
    "        chat_history.append((message, bot_message))\n",
    "        time.sleep(2)\n",
    "        return \"\", chat_history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3018490456.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    pipe = pipeline(\"\",object-detection model=\"facebook/detr-resnet-50\")\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"\",object-detection model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "image = Image.open(\"football-match.jpg\")\n",
    "\n",
    "# Run the segmentation pipeline\n",
    "segmentation_results = pipe(image)\n",
    "\n",
    "\n",
    "# Print segmentation results\n",
    "for result in segmentation_results:\n",
    "    if result['score'] > 0.89:\n",
    "        print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_add(a):\n",
    "    ll=dict()\n",
    "    for i in a:\n",
    "        if i['score'] > 0.89:\n",
    "            if i['label'] not in ll.keys():\n",
    "                ll[i['label']] = 1\n",
    "            else:\n",
    "                ll[i['label']]+=1\n",
    "    return \"there are \\n\"+', \\n'.join([str(i[1])+' '+i[0]  for i  in ll.items() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=reduce_add(segmentation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are \n",
      "1 sports ball, \n",
      "4 person\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kar911/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN='hf_fytqFOBCexHEJOGMFpFHAiUrvnWPaeRU'\n",
    "# repo_id =\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "repo_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "hf = HuggingFaceEndpoint(repo_id=repo_id, huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)\n",
    "\n",
    "llm=ChatHuggingFace(llm=hf)\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "#         ),\n",
    "#         MessagesPlaceholder(variable_name=\"messages\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Passs(Exception):\n",
    "    def __init__(self,message = \"The out put is not generrated\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN='hf_fytqFOBCexHEJOGMFpFHAiUrvnWPaeRU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kar911/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint,HuggingFacePipeline\n",
    "from PIL import Image\n",
    "\n",
    "pipe1 = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "pipe2 = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=100,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "def reduce_add(a):\n",
    "    ll=dict()\n",
    "    for i in a:\n",
    "        if i['score'] > 0.89:\n",
    "            if i['label'] not in ll.keys():\n",
    "                ll[i['label']] = 1\n",
    "            else:\n",
    "                ll[i['label']]+=1\n",
    "    return \"there are \\n\"+', \\n'.join([str(i[1])+' '+i[0]  for i  in ll.items() ])\n",
    "\n",
    "def image_segmentation_tool(image: str):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe1(image)\n",
    "    if reduce_add(segmentation_results) == \"there are \\n\":\n",
    "        raise Passs()\n",
    "    return reduce_add(segmentation_results)\n",
    "\n",
    "def image_caption_tool(image: str):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe2(image)\n",
    "    if segmentation_results[0][\"generated_text\"] == \"\":\n",
    "        raise Passs(\"no result found use different image to create story\") \n",
    "    return segmentation_results[0][\"generated_text\"]\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def story_generation_tool(segmentation_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a storyteller. Based on the following segmentation results, create a story:\n",
    "    {segmentation_results}\n",
    "\n",
    "    Story:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    story = prompt | llm\n",
    "    return story.invoke(input={\"segmentation_results\":segmentation_results})\n",
    "\n",
    "def translation_tool(english_text):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a translator. Translate the following English text to Hindi:\n",
    "    {english_text}\n",
    "\n",
    "    Translation:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    translation = prompt | llm\n",
    "    return translation.invoke(input={\"english_text\": english_text})\n",
    "\n",
    "\n",
    "runnable = RunnableLambda(image_segmentation_tool).with_fallbacks([RunnableLambda(image_caption_tool)])\n",
    "runnable2 = RunnableLambda(story_generation_tool)\n",
    "runnable3 = RunnableLambda(translation_tool)\n",
    "\n",
    "chain = runnable | runnable2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+  \n",
      "| image_segmentation_tool_input |  \n",
      "+-------------------------------+  \n",
      "                *                  \n",
      "                *                  \n",
      "                *                  \n",
      "        +---------------+          \n",
      "        | WithFallbacks |          \n",
      "        +---------------+          \n",
      "                *                  \n",
      "                *                  \n",
      "                *                  \n",
      "+-------------------------------+  \n",
      "| Lambda(story_generation_tool) |  \n",
      "+-------------------------------+  \n",
      "                *                  \n",
      "                *                  \n",
      "                *                  \n",
      "+------------------------------+   \n",
      "| story_generation_tool_output |   \n",
      "+------------------------------+   \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "एक प्रसन्नतम जगत में जीवन के साथ-साथ से राहों पर सुनने वाले व्यवसाय के साथ-साथ दो ट्रक्स और दो हाइड्रोपланे राजते थे। ये शक्तिशाली वाहनें आर्थिक जीवन के रहने वाले साथ-साथ गुड्स और लोगों को एक स्थान से दूसरे तक पहुंचाते थे। \n",
      "\n",
      "    दोनों ट्रक्स, बेसिए और बेटी, पलती-पलती राहों पर बहुत सालों से साथ रहते थे। अपने महान व्यवसाय को धन्य माये से लिये अपने शराब से बजते थे, जिससे उन्होंने एक स्थान से अन्य स्थान तक अपना महत्वपूर्ण साथ लिया था। बेसिए एक स्थाण्डर्द ट्राक थी, उसका छत्र केसों और बरिलें से भरा था, जैसे \n"
     ]
    }
   ],
   "source": [
    "print(runnable3.invoke(\"\"\"In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another. \n",
    "\n",
    "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious. \n",
    " \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_generation_tool(segmentation_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a storyteller. Based on the following segmentation results, create a story:\n",
    "    {segmentation_results}\n",
    "\n",
    "    Story:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    story = prompt | llm\n",
    "    return story.invoke(input={\"segmentation_results\":segmentation_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    In the heart of the city, nestled between towering buildings and bustling streets, was a small, quiet park. The park was a haven for those seeking a moment of peace and tranquility amidst the chaos of urban life. And on this particular sunny afternoon, it was the perfect setting for a friendly game of catch.\n",
      "    \n",
      "    Four friends, all wearing bright smiles and relaxed expressions, gathered in the park. They had known each other since childhood, and their bond had only grown stronger with time. Each one carried a unique energy that added to the group's dynamic.\n",
      "    \n",
      "    John, the jokester, always had a quip ready to lighten the mood. He tossed the sports ball to Tom, the athlete, who caught it effortlessly with one hand. Tom, in turn, threw the ball back to John, who leapt in the air to catch it. The ball soared through the air, its bright colors glinting in the sunlight.\n",
      "    \n",
      "    Sarah, the thoughtful one, watched the game with a contented smile. She was the glue that held the group together, always offering words of encouragement and support. And when it was her turn, she caught the ball with ease, her gentle touch ensuring that the game continued without a hitch.\n",
      "    \n",
      "    Lastly, there was Mark, the quiet one. He preferred to observe the world around him, taking in every detail with a keen eye. But when it was his turn, he threw the ball with just as much enthusiasm as the others, his quiet demeanor belaying his inner excitement.\n",
      "    \n",
      "    And so, the game of catch continued, the ball passing from hand to hand, the laughter and banter filling the air. It was a simple moment, but one that brought joy and connection to these four friends. And as the sun began to set, they packed up their things and went their separate ways, already looking forward to their next game in the quiet little park.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"football-match.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': 'football-match.jpg'}, 'name': 'RunnableSequence', 'tags': [], 'run_id': 'f8d4b471-d5eb-44b7-825a-521db3301bb5', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'football-match.jpg'}, 'name': 'RunnableWithFallbacks', 'tags': ['seq:step:1'], 'run_id': '4e3d063f-768c-4cd3-af8a-7ea0a633fa5e', 'metadata': {}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5']}\n",
      "{'event': 'on_chain_start', 'data': {}, 'name': 'image_segmentation_tool', 'tags': [], 'run_id': 'a2ce2e96-5052-4410-bfff-61be5ad552d5', 'metadata': {}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5', '4e3d063f-768c-4cd3-af8a-7ea0a633fa5e']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'a2ce2e96-5052-4410-bfff-61be5ad552d5', 'name': 'image_segmentation_tool', 'tags': [], 'metadata': {}, 'data': {'chunk': 'there are \\n1 sports ball, \\n4 person'}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5', '4e3d063f-768c-4cd3-af8a-7ea0a633fa5e']}\n",
      "{'event': 'on_chain_start', 'data': {}, 'name': 'story_generation_tool', 'tags': ['seq:step:2'], 'run_id': '74df872e-ba70-41f6-bed6-6418a2f5e106', 'metadata': {}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5']}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'there are \\n1 sports ball, \\n4 person', 'input': 'football-match.jpg'}, 'run_id': 'a2ce2e96-5052-4410-bfff-61be5ad552d5', 'name': 'image_segmentation_tool', 'tags': [], 'metadata': {}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5', '4e3d063f-768c-4cd3-af8a-7ea0a633fa5e']}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'there are \\n1 sports ball, \\n4 person', 'input': 'football-match.jpg'}, 'run_id': '4e3d063f-768c-4cd3-af8a-7ea0a633fa5e', 'name': 'RunnableWithFallbacks', 'tags': ['seq:step:1'], 'metadata': {}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5']}\n",
      "{'event': 'on_chain_stream', 'run_id': '74df872e-ba70-41f6-bed6-6418a2f5e106', 'name': 'story_generation_tool', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': \"\\n    In the heart of the city, nestled between towering buildings and bustling streets, was a small, quiet park. The park was a haven for those seeking a moment of peace and tranquility amidst the chaos of urban life. And on this particular sunny afternoon, it was the perfect setting for a friendly game of catch.\\n    \\n    Four friends, all wearing bright smiles and relaxed expressions, gathered in the park. They had known each other since childhood, and their bond had only grown stronger with time. Each one carried a unique energy that added to the group's dynamic.\\n    \\n    John, the jokester, always had a quip ready to lighten the mood. He tossed the sports ball to Tom, the athlete, who caught it effortlessly with one hand. Tom, in turn, threw the ball back to John, who leapt in the air to catch it. The ball soared through the air, its bright colors glinting in the sunlight.\\n    \\n    Sarah, the thoughtful one, watched the game with a contented smile. She was the glue that held the group together, always offering words of encouragement and support. And when it was her turn, she caught the ball with ease, her gentle touch ensuring that the game continued without a hitch.\\n    \\n    Lastly, there was Mark, the quiet one. He preferred to observe the world around him, taking in every detail with a keen eye. But when it was his turn, he threw the ball with just as much enthusiasm as the others, his quiet demeanor belaying his inner excitement.\\n    \\n    And so, the game of catch continued, the ball passing from hand to hand, the laughter and banter filling the air. It was a simple moment, but one that brought joy and connection to these four friends. And as the sun began to set, they packed up their things and went their separate ways, already looking forward to their next game in the quiet little park.\"}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'f8d4b471-d5eb-44b7-825a-521db3301bb5', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'chunk': \"\\n    In the heart of the city, nestled between towering buildings and bustling streets, was a small, quiet park. The park was a haven for those seeking a moment of peace and tranquility amidst the chaos of urban life. And on this particular sunny afternoon, it was the perfect setting for a friendly game of catch.\\n    \\n    Four friends, all wearing bright smiles and relaxed expressions, gathered in the park. They had known each other since childhood, and their bond had only grown stronger with time. Each one carried a unique energy that added to the group's dynamic.\\n    \\n    John, the jokester, always had a quip ready to lighten the mood. He tossed the sports ball to Tom, the athlete, who caught it effortlessly with one hand. Tom, in turn, threw the ball back to John, who leapt in the air to catch it. The ball soared through the air, its bright colors glinting in the sunlight.\\n    \\n    Sarah, the thoughtful one, watched the game with a contented smile. She was the glue that held the group together, always offering words of encouragement and support. And when it was her turn, she caught the ball with ease, her gentle touch ensuring that the game continued without a hitch.\\n    \\n    Lastly, there was Mark, the quiet one. He preferred to observe the world around him, taking in every detail with a keen eye. But when it was his turn, he threw the ball with just as much enthusiasm as the others, his quiet demeanor belaying his inner excitement.\\n    \\n    And so, the game of catch continued, the ball passing from hand to hand, the laughter and banter filling the air. It was a simple moment, but one that brought joy and connection to these four friends. And as the sun began to set, they packed up their things and went their separate ways, already looking forward to their next game in the quiet little park.\"}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': \"\\n    In the heart of the city, nestled between towering buildings and bustling streets, was a small, quiet park. The park was a haven for those seeking a moment of peace and tranquility amidst the chaos of urban life. And on this particular sunny afternoon, it was the perfect setting for a friendly game of catch.\\n    \\n    Four friends, all wearing bright smiles and relaxed expressions, gathered in the park. They had known each other since childhood, and their bond had only grown stronger with time. Each one carried a unique energy that added to the group's dynamic.\\n    \\n    John, the jokester, always had a quip ready to lighten the mood. He tossed the sports ball to Tom, the athlete, who caught it effortlessly with one hand. Tom, in turn, threw the ball back to John, who leapt in the air to catch it. The ball soared through the air, its bright colors glinting in the sunlight.\\n    \\n    Sarah, the thoughtful one, watched the game with a contented smile. She was the glue that held the group together, always offering words of encouragement and support. And when it was her turn, she caught the ball with ease, her gentle touch ensuring that the game continued without a hitch.\\n    \\n    Lastly, there was Mark, the quiet one. He preferred to observe the world around him, taking in every detail with a keen eye. But when it was his turn, he threw the ball with just as much enthusiasm as the others, his quiet demeanor belaying his inner excitement.\\n    \\n    And so, the game of catch continued, the ball passing from hand to hand, the laughter and banter filling the air. It was a simple moment, but one that brought joy and connection to these four friends. And as the sun began to set, they packed up their things and went their separate ways, already looking forward to their next game in the quiet little park.\", 'input': 'there are \\n1 sports ball, \\n4 person'}, 'run_id': '74df872e-ba70-41f6-bed6-6418a2f5e106', 'name': 'story_generation_tool', 'tags': ['seq:step:2'], 'metadata': {}, 'parent_ids': ['f8d4b471-d5eb-44b7-825a-521db3301bb5']}\n",
      "{'event': 'on_chain_end', 'data': {'output': \"\\n    In the heart of the city, nestled between towering buildings and bustling streets, was a small, quiet park. The park was a haven for those seeking a moment of peace and tranquility amidst the chaos of urban life. And on this particular sunny afternoon, it was the perfect setting for a friendly game of catch.\\n    \\n    Four friends, all wearing bright smiles and relaxed expressions, gathered in the park. They had known each other since childhood, and their bond had only grown stronger with time. Each one carried a unique energy that added to the group's dynamic.\\n    \\n    John, the jokester, always had a quip ready to lighten the mood. He tossed the sports ball to Tom, the athlete, who caught it effortlessly with one hand. Tom, in turn, threw the ball back to John, who leapt in the air to catch it. The ball soared through the air, its bright colors glinting in the sunlight.\\n    \\n    Sarah, the thoughtful one, watched the game with a contented smile. She was the glue that held the group together, always offering words of encouragement and support. And when it was her turn, she caught the ball with ease, her gentle touch ensuring that the game continued without a hitch.\\n    \\n    Lastly, there was Mark, the quiet one. He preferred to observe the world around him, taking in every detail with a keen eye. But when it was his turn, he threw the ball with just as much enthusiasm as the others, his quiet demeanor belaying his inner excitement.\\n    \\n    And so, the game of catch continued, the ball passing from hand to hand, the laughter and banter filling the air. It was a simple moment, but one that brought joy and connection to these four friends. And as the sun began to set, they packed up their things and went their separate ways, already looking forward to their next game in the quiet little park.\"}, 'run_id': 'f8d4b471-d5eb-44b7-825a-521db3301bb5', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "async for i in chain.astream_events(\"football-match.jpg\",version=\"v2\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-50' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-51' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-52' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-53' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-54' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-55' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-56' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-57' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-58' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-59' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-60' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-61' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-62' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-63' coro=<Runnable.abatch_as_completed.<locals>.ainvoke() done, defined at /home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:786> exception=NameError(\"name 'pipe5' is not defined\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 797, in ainvoke\n",
      "    out = await self.ainvoke(input, config, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3996, in ainvoke\n",
      "    return await self._acall_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1650, in _acall_with_config\n",
      "    output = await coro\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3943, in _ainvoke\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3913, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3907, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_12707/3240636366.py\", line 6, in generate_tone1\n",
      "    d = pipe5(note)\n",
      "NameError: name 'pipe5' is not defined\n",
      "/usr/lib/python3.10/importlib/__init__.py:126: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.10/importlib/__init__.py:126: RuntimeWarning: coroutine 'consume_data' was never awaited\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "title = \"Image to short Story Generator\"\n",
    "description = \"\"\"\n",
    "Upload an image, and this app will generate a short story based on the image.\n",
    "\"\"\"\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_img=chain.invoke(input_img)\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(type='pil'), \"textarea\",title=title,\n",
    "    description=description,live=True\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=pipe5(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.9485516e-06, -3.5621350e-05, -2.8692162e-05, ...,\n",
       "        4.8048583e-05,  3.1334428e-05, -9.0521671e-06], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[\"audio\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at facebook/mms-tts-eng were not used when initializing VitsModel: ['flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v']\n",
      "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-eng and are newly initialized: ['flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipe4 = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-hin\")\n",
    "\n",
    "pipe5 = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_voices():\n",
    "    voices = await edge_tts.list_voices()\n",
    "    return {f\"{v['ShortName']} - {v['Locale']} ({v['Gender']})\": v['ShortName'] for v in voices}\n",
    "\n",
    "# Text-to-speech function\n",
    "async def text_to_speech(text, voice, rate, pitch):\n",
    "    if not text.strip():\n",
    "        return None, gr.Warning(\"Please enter text to convert.\")\n",
    "    if not voice:\n",
    "        return None, gr.Warning(\"Please select a voice.\")\n",
    "    \n",
    "    voice_short_name = voice.split(\" - \")[0]\n",
    "    rate_str = f\"{rate:+d}%\"\n",
    "    pitch_str = f\"{pitch:+d}Hz\"\n",
    "    communicate = edge_tts.Communicate(text, voice_short_name, rate=rate_str, pitch=pitch_str)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "        await communicate.save(tmp_path)\n",
    "    return tmp_path, None\n",
    "\n",
    "# Gradio interface function\n",
    "def tts_interface(text, voice, rate, pitch):\n",
    "    audio, warning = asyncio.run(text_to_speech(text, voice, rate, pitch))\n",
    "    return audio, warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Downloading cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.6/117.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycparser, cffi, soundfile\n",
      "Successfully installed cffi-1.16.0 pycparser-2.22 soundfile-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "# sf.write(\n",
    "#         filename,\n",
    "#         audio.samples,\n",
    "#         samplerate=audio.sample_rate,\n",
    "#         subtype=\"PCM_16\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7871\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/processing_utils.py:574: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "# gr.Audio()\n",
    "def generate_tone(note):\n",
    "    d=pipe5(note)\n",
    "    audio = d[\"audio\"].squeeze()\n",
    "    return (d[\"sampling_rate\"],audio)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate_tone,\n",
    "     gr.Textbox(value=1, label=\"Duration in seconds\"),\n",
    "    gr.Audio(label=\"Generated Audio\",streaming=True,autoplay=True)\n",
    "            # gr.Markdown(label=\"Warning\", visible=False)\n",
    "            ,live=True\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kar911/.cache/huggingface/token\n",
      "Login successful\n",
      "\n",
      "    Once upon a time, in a quaint little village nestled in the heart of a dense forest, there lived a young girl named Lily. She was a kind and gentle soul, always spreading joy and love wherever she went. Her heart was as big as the forest itself, and she had a special friend that embodied her generous spirit.\n",
      "\n",
      "    This friend was a beautiful stuffed animal, shaped like a bear, with a soft, velvety heart embroidered on its chest. Lily had found the bear in the forest, abandoned and alone, and she had taken it in as her own. She named him Marvin, and they became inseparable.\n",
      "\n",
      "    Marvin was more than just a toy to Lily. He was her confidant, her companion, and her source of comfort. Whenever she felt sad or lonely, she would hug Marvin tight and let his warm heart soothe her soul. And whenever she wanted to share her happiness, she would give Marvin a gentle squeeze, and his heart would beat in time with her own.\n",
      "\n",
      "    As the years passed, Lily grew up, but her love for Marvin never wavered. She took him with her on all her adventures, and he was always there to listen to her dreams and aspirations. And whenever she returned home, she would find Marvin waiting for her, with his heart beating softly in the quiet stillness of her room.\n",
      "\n",
      "    The villagers spoke in hushed whispers about the magical bear that Lily carried with her everywhere she went. They said that Marvin's heart held the power to heal all wounds and bring people together. And as Lily grew older and wiser, she realized that the magic of Marvin's heart was not just in its ability to soothe and comfort, but in the love and kindness that it represented.\n",
      "\n",
      "    And so, Lily and Marvin continued their journey together, spreading joy and love wherever they went, and reminding everyone of the power of a heart that beats with kindness and compassion. And the forest, with its gentle breeze and its quiet whispers, watched over them, knowing that they were a testament to the enduring power of love.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"test.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use as backup \"nlpconnect/vit-gpt2-image-captioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'generated_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfootball-match.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m segmentation_results \u001b[38;5;241m=\u001b[39m pipe(image)\n\u001b[0;32m----> 7\u001b[0m \u001b[43msegmentation_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerated_text\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'generated_text'"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "image = Image.open(\"football-match.jpg\")\n",
    "segmentation_results = pipe(image)\n",
    "segmentation_results[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a soccer player kicking a soccer ball \n"
     ]
    }
   ],
   "source": [
    "for i in segmentation_results:\n",
    "    print(i[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a stuffed animal with a heart on it '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Once upon a time, in a land far, far away, there existed four distinct realms, each with its unique characteristics and inhabitants. These realms were known as the Realm of Fire, the Realm of Water, the Realm of Earth, and the Realm of Air.\n",
      "\n",
      "    The Realm of Fire was a land of passion and energy. Its inhabitants were fiery beings, with red and orange skin, and eyes that shone like molten gold. They lived in volcanic mountains and fiery forests, surrounded by lava rivers. Their days were filled with excitement and adventure, as they explored the depths of their fiery world.\n",
      "\n",
      "    The Realm of Water was a tranquil and serene place. Its inhabitants were water creatures, with shimmering blue and green scales, and gills that allowed them to breathe underwater. They lived in vast oceans and crystal-clear lakes, surrounded by coral reefs and underwater gardens. Their days were filled with peace and harmony, as they swam gracefully through their watery world.\n",
      "\n",
      "    The Realm of Earth was a land of stability and growth. Its inhabitants were sturdy beings, with brown and gray skin, and strong, muscular limbs. They lived in rolling hills and lush forests, surrounded by towering mountains and fertile valleys. Their days were filled with hard work and determination, as they tilled the soil and cultivated the land.\n",
      "\n",
      "    The Realm of Air was a land of freedom and creativity. Its inhabitants were ethereal beings, with translucent wings and glowing bodies. They lived in vast open skies and towering clouds, surrounded by endless horizons and colorful rainbows. Their days were filled with endless possibilities, as they soared through the skies and painted the clouds with their vibrant colors.\n",
      "\n",
      "    Despite their differences, the inhabitants of these realms respected and admired one another. They would often gather at the Crossroads, a magical place where the four realms met, to share stories and learn from one another. And so, the Realm of Fire, the Realm of Water, the Realm of Earth, and the Realm of Air lived in harmony, each cherishing the unique gifts that their realm offered them.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"The-Foolish.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
      "\n",
      "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
      "\n",
      "    The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\n",
      "\n",
      "    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\n",
      "\n",
      "    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\n",
      "\n",
      "    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"plan.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HuggingFaceEndpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"facebook/mms-tts-eng\"\n",
    "\"facebook/mms-tts-hin\"\n",
    "\"swarpatil9825/opus-mt-en-hi\"\n",
    "\"Helsinki-NLP/opus-mt-en-hi\"\n",
    "\"barghavani/English_to_Hindi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,TextSplitter\n",
    "c=RecursiveCharacterTextSplitter(separators=[\".\"],chunk_size= 20,    chunk_overlap= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe3 = pipeline(\"translation\", model=\"swarpatil9825/opus-mt-en-hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "ff=pipe3(\"In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,TextSplitter\n",
    "c=RecursiveCharacterTextSplitter(separators=[\".\"],chunk_size= 20,    chunk_overlap= 0)\n",
    "g=c.split_text(\"\"\"\n",
    "In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
    "\n",
    "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
    "\n",
    "    The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\n",
    "\n",
    "    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\n",
    "\n",
    "    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\n",
    "\n",
    "    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for i in runnable4.abatch_as_completed(inputs=[*g]):\n",
    "    tet.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tet.sort(key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kar911/.cache/huggingface/token\n",
      "Login successful\n",
      "In the heart of the bustling city, there was a small, quiet park where people went to escape the noise and chaos of urban life. The park was surrounded by towering buildings, but in its center, there was a lush green field where children played and adults relaxed.\n",
      "\n",
      "One sunny afternoon, four friends, John, Mike, Tom, and Dave, gathered around the park's only football goalpost. They had grown up together, playing football in this very park since they were young boys. Now, as men, they still found joy in the simple game that had brought them together for so many years.\n",
      "\n",
      "John, the tallest and strongest of the group, was the goalkeeper. Mike, the fastest, was the striker. Tom, the most agile, was the midfielder, and Dave, the most creative, was the playmaker. They had their roles down pat, and they knew each other's strengths and weaknesses like the back of their hands.\n",
      "\n",
      "As they prepared to start their game, the sun began to set, casting long shadows over the field. The air was filled with the sound of birds chirping, children laughing, and the occasional honk of a car passing by. The friends took a moment to appreciate the beauty of the scene before them, knowing that this was a moment they would cherish for years to come.\n",
      "\n",
      "With a nod from John, the game began. Mike dribbled the ball towards the goalpost, evading Tom's attempts to steal it away. Dave made a clever pass to Tom, who scored a goal with a powerful kick. The friends cheered and high-fived each other, their faces glowing with happiness.\n",
      "\n",
      "The game continued, with each friend taking turns to showcase their skills. John made impressive saves, Mike scored stunning goals, Tom made incredible tackles, and Dave came up with ingenious plays. They laughed and joked, their spirits lifted by the simple pleasure of playing football together.\n",
      "\n",
      "As the sun set and the stars began to twinkle in the sky, the friends packed up their things and said their goodbyes. They knew that life would take them in different directions, but they also knew that they would always have this park, this game, and each other. And so, they went their separate ways, already looking forward to their next football game in the quiet, green heart of the city.</s>"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "HUGGINGFACEHUB_API_TOKEN='hf_fytqFOBCexHEJOGMFpFHAiUrvnWPaeRU'\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a storyteller. Based on the following segmentation results, create a story:\n",
    "{segmentation_results}\n",
    "\n",
    "Story:\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "segmentation_results=\"\"\"there are \n",
    "1 football, \n",
    "4 person\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "story = prompt | llm\n",
    "chunks = []\n",
    "async for e in story.astream(input={\"segmentation_results\":segmentation_results}):\n",
    "    chunks.append(e)\n",
    "    print(e,end=\"\",flush=True)\n",
    "# llm.astream(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "chain.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 541, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1526, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 657, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 783, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "  File \"/tmp/ipykernel_10839/1810914755.py\", line 77, in sepia\n",
      "    async for e in chain.astream(input={\"segmentation_results\":input_img}):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2907, in astream\n",
      "    async for chunk in self.atransform(input_aiter(), config, **kwargs):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2890, in atransform\n",
      "    async for chunk in self._atransform_stream_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1978, in _atransform_stream_with_config\n",
      "    chunk = cast(Output, await py_anext(iterator))\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2860, in _atransform\n",
      "    async for output in final_pipeline:\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4193, in atransform\n",
      "    async for output in self._atransform_stream_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1931, in _atransform_stream_with_config\n",
      "    final_input: Optional[Input] = await py_anext(input_for_tracing, None)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 65, in anext_impl\n",
      "    return await __anext__(iterator)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 100, in tee_peer\n",
      "    item = await iterator.__anext__()\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1215, in atransform\n",
      "    async for output in self.astream(final, config, **kwargs):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/fallbacks.py\", line 540, in astream\n",
      "    raise first_error\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/fallbacks.py\", line 528, in astream\n",
      "    chunk = await cast(Awaitable[Output], py_anext(stream))\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4210, in astream\n",
      "    async for chunk in self.atransform(input_aiter(), config, **kwargs):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4193, in atransform\n",
      "    async for output in self._atransform_stream_with_config(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1978, in _atransform_stream_with_config\n",
      "    chunk = cast(Output, await py_anext(iterator))\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4162, in _atransform\n",
      "    output = await acall_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4137, in f\n",
      "    return await run_in_executor(config, func, *args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4131, in func\n",
      "    return call_func_with_variable_args(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 380, in call_func_with_variable_args\n",
      "    return func(input, **kwargs)  # type: ignore[call-arg]\n",
      "  File \"/tmp/ipykernel_10839/3510513901.py\", line 30, in image_segmentation_tool\n",
      "    segmentation_results = pipe1(image)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/object_detection.py\", line 104, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1243, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1249, in run_single\n",
      "    model_inputs = self.preprocess(inputs, **preprocess_params)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/object_detection.py\", line 107, in preprocess\n",
      "    image = load_image(image, timeout=timeout)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/image_utils.py\", line 335, in load_image\n",
      "    raise ValueError(\n",
      "ValueError: Incorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import asyncio\n",
    "\n",
    "title = \"Image to short Story Generator\"\n",
    "description = \"\"\"\n",
    "Upload an image, and this app will generate a short story based on the image.\n",
    "\"\"\"\n",
    "\n",
    "# v=0\n",
    "\n",
    "\n",
    "\n",
    "import asyncio\n",
    "Full=[]\n",
    "# Declare chunks as global\n",
    "chunks = []\n",
    "\n",
    "async def sepia(input_img):\n",
    "    global chunks\n",
    "    v = 0\n",
    "    Full = []\n",
    "    async for e in story.astream(input={\"segmentation_results\": input_img}):\n",
    "        if v // 60 == 0:\n",
    "            \"\".join(chunks)\n",
    "            chunks.clear()\n",
    "        chunks.append(e)\n",
    "        Full.extend(chunks)\n",
    "        v += 1\n",
    "        yield \"\".join(Full)\n",
    "\n",
    "async def sepia1(input_img):\n",
    "    global chunks\n",
    "    v = 0\n",
    "    Full = []\n",
    "    async for e in story.astream(input={\"segmentation_results\": input_img}):\n",
    "        if v // 30 == 0:\n",
    "            \"\".join(chunks)\n",
    "            chunks.clear()\n",
    "        chunks.append(e)\n",
    "        Full.extend(chunks)\n",
    "        v += 1\n",
    "        yield \"\".join(Full)\n",
    "\n",
    "async def combined_sepia_functions(input_img):\n",
    "    sepia_gen = sepia(input_img)\n",
    "    sepia1_gen = sepia1(input_img)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            result1 = await sepia_gen.__anext__()\n",
    "        except StopAsyncIteration:\n",
    "            result1 = None\n",
    "\n",
    "        try:\n",
    "            result2 = await sepia1_gen.__anext__()\n",
    "        except StopAsyncIteration:\n",
    "            result2 = None\n",
    "\n",
    "        if result1 is None and result2 is None:\n",
    "            break\n",
    "\n",
    "        yield (result1, result2)\n",
    "\n",
    "# Example usage:\n",
    "# async for text, audio in combined_sepia_functions(input_img):\n",
    "#     print(\"Text:\", text)\n",
    "#     print(\"Audio:\", audio)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# async for text, audio in combined_sepia_functions(input_img):\n",
    "#     print(\"Text:\", text)\n",
    "#     print(\"Audio:\", audio)\n",
    "\n",
    "async def sepia(input_img):\n",
    "    v=0\n",
    "    async for e in chain.astream(input={\"segmentation_results\":input_img}):\n",
    "        if v//60== 0:\n",
    "            \"\".join(chunks)\n",
    "            chunks.clear()\n",
    "        chunks.append(e)\n",
    "        Full.extend(chunks)\n",
    "        v=+1\n",
    "        yield \"\".join(Full)\n",
    "        # yield e\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(type='pil'), \"textarea\",title=title,\n",
    "    description=description,live=True\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-tts-eng were not used when initializing VitsModel: ['flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v']\n",
      "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-eng and are newly initialized: ['flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipe4 = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-hin\")\n",
    "\n",
    "pipe5 = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/processing_utils.py:574: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d= pipe5(note)\n",
    "    return (d[\"sampling_rate\"],d[\"audio\"].squeeze())\n",
    "\n",
    "\n",
    "async def generate_tone(note):\n",
    "    d=  pipe5(note)\n",
    "    return (d[\"sampling_rate\"],d[\"audio\"].squeeze())\n",
    "runabl=RunnableLambda(func=generate_tone1,afunc=generate_tone)\n",
    "runabl.astream(\"Default implementation of astream, which calls ainvoke. Subclasses should override this method if they support streaming output.\")\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "# gr.Audio()\n",
    "# def generate_tone(note):\n",
    "#     d=pipe5(note)\n",
    "#     audio = d[\"audio\"].squeeze()\n",
    "#     return (d[\"sampling_rate\"],audio)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,TextSplitter\n",
    "\n",
    "# g=c.split_text(\"\"\"\n",
    "# In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
    "\n",
    "#     The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
    "\n",
    "#     The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\n",
    "\n",
    "#     Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\n",
    "\n",
    "#     Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\n",
    "\n",
    "#     And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\n",
    "# \"\"\")\n",
    "\n",
    "async def generate_tone1(text):\n",
    "    c=RecursiveCharacterTextSplitter(separators=[\".\"],chunk_size= 20,    chunk_overlap= 0)\n",
    "    async for i in runabl.astream(text):\n",
    "        return i\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate_tone1,\n",
    "     gr.Textbox( label=\"Duration in seconds\"),\n",
    "    gr.Audio(label=\"Generated Audio\",streaming=True,autoplay=True)\n",
    "            # gr.Markdown(label=\"Warning\", visible=False)\n",
    "            ,live=True\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone1, afunc=generate_tone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runabl.astream()\n",
    "runabl.abatch_as_completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "\n",
    "async def generate_tone2(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    return await generate_tone2(note)\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone1, afunc=generate_tone)\n",
    "\n",
    "async def generate_tone1(text):\n",
    "    c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "    chunks = c.split_text(text=text)\n",
    "    async for i in runabl.abatch_as_completed(chunks):\n",
    "        yield (16000,i)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_tone1,\n",
    "    inputs=gr.Textbox(label=\"Duration in seconds\"),\n",
    "    outputs=gr.Audio(label=\"Generated Audio\", streaming=True, autoplay=True),\n",
    "    live=True\n",
    ")\n",
    "\n",
    "demo.output_components.end\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "    # kÄrtiːk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_tone2(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    yield generate_tone2(note)\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone1, afunc=generate_tone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
    "\n",
    "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m c \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m], chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39msplit_text(text\u001b[38;5;241m=\u001b[39mtext)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m runabl\u001b[38;5;241m.\u001b[39mbatch_as_completed(chunks):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:709\u001b[0m, in \u001b[0;36mRunnable.batch_as_completed\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         done, futures \u001b[38;5;241m=\u001b[39m wait(futures, return_when\u001b[38;5;241m=\u001b[39mFIRST_COMPLETED)\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m done:\n\u001b[0;32m--> 709\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:691\u001b[0m, in \u001b[0;36mRunnable.batch_as_completed.<locals>.invoke\u001b[0;34m(i, input, config)\u001b[0m\n\u001b[1;32m    689\u001b[0m         out \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (i, out)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3976\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3974\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3986\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1598\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1595\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1597\u001b[0m         Output,\n\u001b[0;32m-> 1598\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1606\u001b[0m     )\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1608\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3844\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3842\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3844\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3845\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3847\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mgenerate_tone1\u001b[0;34m(note)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_tone1\u001b[39m(note):\n\u001b[0;32m----> 6\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mpipe5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnote\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampling_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/text_to_audio.py:182\u001b[0m, in \u001b[0;36mTextToAudioPipeline.__call__\u001b[0;34m(self, text_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m        - **sampling_rate** (`int`) -- The sampling rate of the generated audio waveform.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1243\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         )\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1250\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1249\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1250\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1150\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1149\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/pipelines/text_to_audio.py:152\u001b[0m, in \u001b[0;36mTextToAudioPipeline._forward\u001b[0;34m(self, model_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generate_kwargs):\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using the `TextToAudioPipeline` with a forward-only model, but `generate_kwargs` is non empty.\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124m                         For forward-only TTA models, please use `forward_params` instead of of\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124m                         `generate_kwargs`. For reference, here are the `generate_kwargs` used here:\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124m                         \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_kwargs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[0;32m--> 152\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# in that case, the output is a spectrogram that needs to be converted into a waveform\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocoder(output)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/models/vits/modeling_vits.py:1414\u001b[0m, in \u001b[0;36mVitsModel.forward\u001b[0;34m(self, input_ids, attention_mask, speaker_id, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining of VITS is not supported yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1414\u001b[0m text_encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m text_encoder_output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m text_encoder_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1423\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/transformers/models/vits/modeling_vits.py:1211\u001b[0m, in \u001b[0;36mVitsTextEncoder.forward\u001b[0;34m(self, input_ids, padding_mask, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1204\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1210\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], VitsTextEncoderOutput]:\n\u001b[0;32m-> 1211\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m   1213\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1214\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1215\u001b[0m         padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1219\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1220\u001b[0m     )\n\u001b[1;32m   1222\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m encoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "chunks = c.split_text(text=text)\n",
    "for i in runabl.abatch_as_completed(chunks):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RecursiveCharacterTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c\u001b[38;5;241m=\u001b[39m\u001b[43mRecursiveCharacterTextSplitter\u001b[49m(separators\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m],chunk_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,    chunk_overlap\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RecursiveCharacterTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "c=RecursiveCharacterTextSplitter(separators=[\".\"],chunk_size= 20,    chunk_overlap= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g\u001b[38;5;241m=\u001b[39m\u001b[43mc\u001b[49m\u001b[38;5;241m.\u001b[39msplit_text(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mIn a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\u001b[39m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\u001b[39m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\u001b[39m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\u001b[39m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\u001b[39m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m g\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "g=c.split_text(\"\"\"\n",
    "In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
    "\n",
    "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
    "\n",
    "    The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\n",
    "\n",
    "    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\n",
    "\n",
    "    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\n",
    "\n",
    "    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\n",
    "\"\"\")\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once',\n",
       " ' upon',\n",
       " ' a',\n",
       " ' time',\n",
       " ',',\n",
       " ' in',\n",
       " ' a',\n",
       " ' small',\n",
       " ' village',\n",
       " ' nest',\n",
       " 'led',\n",
       " ' between',\n",
       " ' rolling',\n",
       " ' hills',\n",
       " ' and',\n",
       " ' a',\n",
       " ' spark',\n",
       " 'ling',\n",
       " ' river',\n",
       " ',',\n",
       " ' there',\n",
       " ' was',\n",
       " ' a',\n",
       " ' group',\n",
       " ' of',\n",
       " ' four',\n",
       " ' friends',\n",
       " ' named',\n",
       " ' Tom',\n",
       " ',',\n",
       " ' Jerry',\n",
       " ',',\n",
       " ' Mike',\n",
       " ',',\n",
       " ' and',\n",
       " ' Sam',\n",
       " '.',\n",
       " ' They',\n",
       " ' were',\n",
       " ' in',\n",
       " 'separ',\n",
       " 'able',\n",
       " ' since',\n",
       " ' childhood',\n",
       " ' and',\n",
       " ' shared',\n",
       " ' a',\n",
       " ' common',\n",
       " ' passion',\n",
       " ' for',\n",
       " ' sports',\n",
       " ',',\n",
       " ' especially',\n",
       " ' football',\n",
       " '.',\n",
       " ' Every',\n",
       " ' evening',\n",
       " ',',\n",
       " ' after',\n",
       " ' their',\n",
       " ' cho',\n",
       " 'res',\n",
       " ' were',\n",
       " ' done',\n",
       " ',',\n",
       " ' they',\n",
       " ' would',\n",
       " ' gather',\n",
       " ' in',\n",
       " ' the',\n",
       " ' open',\n",
       " ' field',\n",
       " ' near',\n",
       " ' the',\n",
       " ' village',\n",
       " ' square',\n",
       " ' to',\n",
       " ' play',\n",
       " ' a',\n",
       " ' game',\n",
       " ' of',\n",
       " ' football',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Tom',\n",
       " ',',\n",
       " ' the',\n",
       " ' tall',\n",
       " 'est',\n",
       " ' and',\n",
       " ' strongest',\n",
       " ' of',\n",
       " ' the',\n",
       " ' group',\n",
       " ',',\n",
       " ' was',\n",
       " ' the',\n",
       " ' team',\n",
       " \"'\",\n",
       " 's',\n",
       " ' center',\n",
       " ' forward',\n",
       " '.',\n",
       " ' Jerry',\n",
       " ',',\n",
       " ' the',\n",
       " ' smallest',\n",
       " ' but',\n",
       " ' quick',\n",
       " 'est',\n",
       " ',',\n",
       " ' was',\n",
       " ' the',\n",
       " ' w',\n",
       " 'inger',\n",
       " '.',\n",
       " ' Mike',\n",
       " ',',\n",
       " ' the',\n",
       " ' most',\n",
       " ' ag',\n",
       " 'ile',\n",
       " ',',\n",
       " ' was',\n",
       " ' the',\n",
       " ' goal',\n",
       " 'keeper',\n",
       " ',',\n",
       " ' and',\n",
       " ' Sam',\n",
       " ',',\n",
       " ' the',\n",
       " ' most',\n",
       " ' determined',\n",
       " ',',\n",
       " ' was',\n",
       " ' the',\n",
       " ' mid',\n",
       " 'f',\n",
       " 'iel',\n",
       " 'der',\n",
       " '.',\n",
       " ' Together',\n",
       " ',',\n",
       " ' they',\n",
       " ' formed',\n",
       " ' an',\n",
       " ' un',\n",
       " 'beat',\n",
       " 'able',\n",
       " ' team',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'The',\n",
       " ' sun',\n",
       " ' would',\n",
       " ' set',\n",
       " ',',\n",
       " ' casting',\n",
       " ' long',\n",
       " ' shadows',\n",
       " ' over',\n",
       " ' the',\n",
       " ' field',\n",
       " ',',\n",
       " ' as',\n",
       " ' they',\n",
       " ' played',\n",
       " ' their',\n",
       " ' game',\n",
       " '.',\n",
       " ' They',\n",
       " ' che',\n",
       " 'ered',\n",
       " ' and',\n",
       " ' laughed',\n",
       " ',',\n",
       " ' their',\n",
       " ' joy',\n",
       " ' echo',\n",
       " 'ing',\n",
       " ' through',\n",
       " ' the',\n",
       " ' village',\n",
       " '.',\n",
       " ' The',\n",
       " ' vill',\n",
       " 'agers',\n",
       " ' would',\n",
       " ' watch',\n",
       " ' from',\n",
       " ' their',\n",
       " ' windows',\n",
       " ',',\n",
       " ' smiling',\n",
       " ' at',\n",
       " ' the',\n",
       " ' sight',\n",
       " ' of',\n",
       " ' their',\n",
       " ' young',\n",
       " ' friends',\n",
       " ' playing',\n",
       " ' with',\n",
       " ' such',\n",
       " ' enthusiasm',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'One',\n",
       " ' day',\n",
       " ',',\n",
       " ' as',\n",
       " ' they',\n",
       " ' were',\n",
       " ' playing',\n",
       " ' their',\n",
       " ' usual',\n",
       " ' game',\n",
       " ',',\n",
       " ' a',\n",
       " ' travel',\n",
       " 'er',\n",
       " ' passing',\n",
       " ' through',\n",
       " ' the',\n",
       " ' village',\n",
       " ' stopped',\n",
       " ' to',\n",
       " ' watch',\n",
       " '.',\n",
       " ' He',\n",
       " ' was',\n",
       " ' amaz',\n",
       " 'ed',\n",
       " ' by',\n",
       " ' their',\n",
       " ' team',\n",
       " 'work',\n",
       " ' and',\n",
       " ' the',\n",
       " ' sheer',\n",
       " ' joy',\n",
       " ' they',\n",
       " ' derived',\n",
       " ' from',\n",
       " ' playing',\n",
       " ' football',\n",
       " '.',\n",
       " ' He',\n",
       " ' approached',\n",
       " ' them',\n",
       " ' and',\n",
       " ' offered',\n",
       " ' to',\n",
       " ' take',\n",
       " ' them',\n",
       " ' to',\n",
       " ' the',\n",
       " ' nearby',\n",
       " ' city',\n",
       " ' to',\n",
       " ' play',\n",
       " ' against',\n",
       " ' other',\n",
       " ' teams',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Tom',\n",
       " ',',\n",
       " ' Jerry',\n",
       " ',',\n",
       " ' Mike',\n",
       " ',',\n",
       " ' and',\n",
       " ' Sam',\n",
       " ' were',\n",
       " ' thr',\n",
       " 'illed',\n",
       " ' at',\n",
       " ' the',\n",
       " ' opportunity',\n",
       " '.',\n",
       " ' They',\n",
       " ' spent',\n",
       " ' days',\n",
       " ' preparing',\n",
       " ' for',\n",
       " ' the',\n",
       " ' tournament',\n",
       " ',',\n",
       " ' practicing',\n",
       " ' every',\n",
       " ' evening',\n",
       " '.',\n",
       " ' Their',\n",
       " ' hard',\n",
       " ' work',\n",
       " ' paid',\n",
       " ' off',\n",
       " ',',\n",
       " ' and',\n",
       " ' they',\n",
       " ' won',\n",
       " ' the',\n",
       " ' tournament',\n",
       " ',',\n",
       " ' bringing',\n",
       " ' glory',\n",
       " ' to',\n",
       " ' their',\n",
       " ' village',\n",
       " '.',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'From',\n",
       " ' that',\n",
       " ' day',\n",
       " ' forward',\n",
       " ',',\n",
       " ' their',\n",
       " ' story',\n",
       " ' spread',\n",
       " ' far',\n",
       " ' and',\n",
       " ' wide',\n",
       " ',',\n",
       " ' insp',\n",
       " 'iring',\n",
       " ' other',\n",
       " ' young',\n",
       " ' children',\n",
       " ' to',\n",
       " ' follow',\n",
       " ' their',\n",
       " ' dreams',\n",
       " ' and',\n",
       " ' play',\n",
       " ' the',\n",
       " ' sport',\n",
       " ' they',\n",
       " ' loved',\n",
       " '.',\n",
       " ' And',\n",
       " ' every',\n",
       " ' evening',\n",
       " ',',\n",
       " ' as',\n",
       " ' the',\n",
       " ' sun',\n",
       " ' set',\n",
       " ' over',\n",
       " ' the',\n",
       " ' village',\n",
       " ',',\n",
       " ' Tom',\n",
       " ',',\n",
       " ' Jerry',\n",
       " ',',\n",
       " ' Mike',\n",
       " ',',\n",
       " ' and',\n",
       " ' Sam',\n",
       " ' would',\n",
       " ' gather',\n",
       " ' in',\n",
       " ' the',\n",
       " ' open',\n",
       " ' field',\n",
       " ',',\n",
       " ' their',\n",
       " ' hearts',\n",
       " ' filled',\n",
       " ' with',\n",
       " ' the',\n",
       " ' joy',\n",
       " ' of',\n",
       " ' football',\n",
       " ' and',\n",
       " ' the',\n",
       " ' memories',\n",
       " ' of',\n",
       " ' their',\n",
       " ' un',\n",
       " 'for',\n",
       " 'get',\n",
       " 'table',\n",
       " ' journey',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinchunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def async_generator():\n",
    "    for i in range(3):\n",
    "        await asyncio.sleep(1)\n",
    "        yield i*i\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async for i in async_generator():\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mloop1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     loop1\u001b[38;5;241m.\u001b[39mrun_until_complete(main())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mloop1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown_asyncgens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# see: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.shutdown_asyncgens\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     loop1\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "loop1 = asyncio.get_event_loop()\n",
    "try:\n",
    "    loop1.run_until_complete(main())\n",
    "finally:\n",
    "    loop1.run_until_complete(loop.shutdown_asyncgens())  # see: https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.shutdown_asyncgens\n",
    "    loop1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/mms-tts-eng were not used when initializing VitsModel: ['flow.flows.0.wavenet.in_layers.0.weight_g', 'flow.flows.0.wavenet.in_layers.0.weight_v', 'flow.flows.0.wavenet.in_layers.1.weight_g', 'flow.flows.0.wavenet.in_layers.1.weight_v', 'flow.flows.0.wavenet.in_layers.2.weight_g', 'flow.flows.0.wavenet.in_layers.2.weight_v', 'flow.flows.0.wavenet.in_layers.3.weight_g', 'flow.flows.0.wavenet.in_layers.3.weight_v', 'flow.flows.0.wavenet.res_skip_layers.0.weight_g', 'flow.flows.0.wavenet.res_skip_layers.0.weight_v', 'flow.flows.0.wavenet.res_skip_layers.1.weight_g', 'flow.flows.0.wavenet.res_skip_layers.1.weight_v', 'flow.flows.0.wavenet.res_skip_layers.2.weight_g', 'flow.flows.0.wavenet.res_skip_layers.2.weight_v', 'flow.flows.0.wavenet.res_skip_layers.3.weight_g', 'flow.flows.0.wavenet.res_skip_layers.3.weight_v', 'flow.flows.1.wavenet.in_layers.0.weight_g', 'flow.flows.1.wavenet.in_layers.0.weight_v', 'flow.flows.1.wavenet.in_layers.1.weight_g', 'flow.flows.1.wavenet.in_layers.1.weight_v', 'flow.flows.1.wavenet.in_layers.2.weight_g', 'flow.flows.1.wavenet.in_layers.2.weight_v', 'flow.flows.1.wavenet.in_layers.3.weight_g', 'flow.flows.1.wavenet.in_layers.3.weight_v', 'flow.flows.1.wavenet.res_skip_layers.0.weight_g', 'flow.flows.1.wavenet.res_skip_layers.0.weight_v', 'flow.flows.1.wavenet.res_skip_layers.1.weight_g', 'flow.flows.1.wavenet.res_skip_layers.1.weight_v', 'flow.flows.1.wavenet.res_skip_layers.2.weight_g', 'flow.flows.1.wavenet.res_skip_layers.2.weight_v', 'flow.flows.1.wavenet.res_skip_layers.3.weight_g', 'flow.flows.1.wavenet.res_skip_layers.3.weight_v', 'flow.flows.2.wavenet.in_layers.0.weight_g', 'flow.flows.2.wavenet.in_layers.0.weight_v', 'flow.flows.2.wavenet.in_layers.1.weight_g', 'flow.flows.2.wavenet.in_layers.1.weight_v', 'flow.flows.2.wavenet.in_layers.2.weight_g', 'flow.flows.2.wavenet.in_layers.2.weight_v', 'flow.flows.2.wavenet.in_layers.3.weight_g', 'flow.flows.2.wavenet.in_layers.3.weight_v', 'flow.flows.2.wavenet.res_skip_layers.0.weight_g', 'flow.flows.2.wavenet.res_skip_layers.0.weight_v', 'flow.flows.2.wavenet.res_skip_layers.1.weight_g', 'flow.flows.2.wavenet.res_skip_layers.1.weight_v', 'flow.flows.2.wavenet.res_skip_layers.2.weight_g', 'flow.flows.2.wavenet.res_skip_layers.2.weight_v', 'flow.flows.2.wavenet.res_skip_layers.3.weight_g', 'flow.flows.2.wavenet.res_skip_layers.3.weight_v', 'flow.flows.3.wavenet.in_layers.0.weight_g', 'flow.flows.3.wavenet.in_layers.0.weight_v', 'flow.flows.3.wavenet.in_layers.1.weight_g', 'flow.flows.3.wavenet.in_layers.1.weight_v', 'flow.flows.3.wavenet.in_layers.2.weight_g', 'flow.flows.3.wavenet.in_layers.2.weight_v', 'flow.flows.3.wavenet.in_layers.3.weight_g', 'flow.flows.3.wavenet.in_layers.3.weight_v', 'flow.flows.3.wavenet.res_skip_layers.0.weight_g', 'flow.flows.3.wavenet.res_skip_layers.0.weight_v', 'flow.flows.3.wavenet.res_skip_layers.1.weight_g', 'flow.flows.3.wavenet.res_skip_layers.1.weight_v', 'flow.flows.3.wavenet.res_skip_layers.2.weight_g', 'flow.flows.3.wavenet.res_skip_layers.2.weight_v', 'flow.flows.3.wavenet.res_skip_layers.3.weight_g', 'flow.flows.3.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.0.weight_g', 'posterior_encoder.wavenet.in_layers.0.weight_v', 'posterior_encoder.wavenet.in_layers.1.weight_g', 'posterior_encoder.wavenet.in_layers.1.weight_v', 'posterior_encoder.wavenet.in_layers.10.weight_g', 'posterior_encoder.wavenet.in_layers.10.weight_v', 'posterior_encoder.wavenet.in_layers.11.weight_g', 'posterior_encoder.wavenet.in_layers.11.weight_v', 'posterior_encoder.wavenet.in_layers.12.weight_g', 'posterior_encoder.wavenet.in_layers.12.weight_v', 'posterior_encoder.wavenet.in_layers.13.weight_g', 'posterior_encoder.wavenet.in_layers.13.weight_v', 'posterior_encoder.wavenet.in_layers.14.weight_g', 'posterior_encoder.wavenet.in_layers.14.weight_v', 'posterior_encoder.wavenet.in_layers.15.weight_g', 'posterior_encoder.wavenet.in_layers.15.weight_v', 'posterior_encoder.wavenet.in_layers.2.weight_g', 'posterior_encoder.wavenet.in_layers.2.weight_v', 'posterior_encoder.wavenet.in_layers.3.weight_g', 'posterior_encoder.wavenet.in_layers.3.weight_v', 'posterior_encoder.wavenet.in_layers.4.weight_g', 'posterior_encoder.wavenet.in_layers.4.weight_v', 'posterior_encoder.wavenet.in_layers.5.weight_g', 'posterior_encoder.wavenet.in_layers.5.weight_v', 'posterior_encoder.wavenet.in_layers.6.weight_g', 'posterior_encoder.wavenet.in_layers.6.weight_v', 'posterior_encoder.wavenet.in_layers.7.weight_g', 'posterior_encoder.wavenet.in_layers.7.weight_v', 'posterior_encoder.wavenet.in_layers.8.weight_g', 'posterior_encoder.wavenet.in_layers.8.weight_v', 'posterior_encoder.wavenet.in_layers.9.weight_g', 'posterior_encoder.wavenet.in_layers.9.weight_v', 'posterior_encoder.wavenet.res_skip_layers.0.weight_g', 'posterior_encoder.wavenet.res_skip_layers.0.weight_v', 'posterior_encoder.wavenet.res_skip_layers.1.weight_g', 'posterior_encoder.wavenet.res_skip_layers.1.weight_v', 'posterior_encoder.wavenet.res_skip_layers.10.weight_g', 'posterior_encoder.wavenet.res_skip_layers.10.weight_v', 'posterior_encoder.wavenet.res_skip_layers.11.weight_g', 'posterior_encoder.wavenet.res_skip_layers.11.weight_v', 'posterior_encoder.wavenet.res_skip_layers.12.weight_g', 'posterior_encoder.wavenet.res_skip_layers.12.weight_v', 'posterior_encoder.wavenet.res_skip_layers.13.weight_g', 'posterior_encoder.wavenet.res_skip_layers.13.weight_v', 'posterior_encoder.wavenet.res_skip_layers.14.weight_g', 'posterior_encoder.wavenet.res_skip_layers.14.weight_v', 'posterior_encoder.wavenet.res_skip_layers.15.weight_g', 'posterior_encoder.wavenet.res_skip_layers.15.weight_v', 'posterior_encoder.wavenet.res_skip_layers.2.weight_g', 'posterior_encoder.wavenet.res_skip_layers.2.weight_v', 'posterior_encoder.wavenet.res_skip_layers.3.weight_g', 'posterior_encoder.wavenet.res_skip_layers.3.weight_v', 'posterior_encoder.wavenet.res_skip_layers.4.weight_g', 'posterior_encoder.wavenet.res_skip_layers.4.weight_v', 'posterior_encoder.wavenet.res_skip_layers.5.weight_g', 'posterior_encoder.wavenet.res_skip_layers.5.weight_v', 'posterior_encoder.wavenet.res_skip_layers.6.weight_g', 'posterior_encoder.wavenet.res_skip_layers.6.weight_v', 'posterior_encoder.wavenet.res_skip_layers.7.weight_g', 'posterior_encoder.wavenet.res_skip_layers.7.weight_v', 'posterior_encoder.wavenet.res_skip_layers.8.weight_g', 'posterior_encoder.wavenet.res_skip_layers.8.weight_v', 'posterior_encoder.wavenet.res_skip_layers.9.weight_g', 'posterior_encoder.wavenet.res_skip_layers.9.weight_v']\n",
      "- This IS expected if you are initializing VitsModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VitsModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of VitsModel were not initialized from the model checkpoint at facebook/mms-tts-eng and are newly initialized: ['flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.0.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.1.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.2.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.in_layers.3.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'flow.flows.3.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.in_layers.9.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.0.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.1.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.10.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.11.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.12.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.13.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.14.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.15.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.2.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.3.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.4.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.5.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.6.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.7.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.8.parametrizations.weight.original1', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original0', 'posterior_encoder.wavenet.res_skip_layers.9.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipe4 = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-hin\")\n",
    "\n",
    "pipe5 = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     generated_audio \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mAudio(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoplay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# duration_input.change(generate_tone1, inputs=duration_input, outputs=generated_audio)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@gr\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_audio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mgreet\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# yield pipe5(\"\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:97\u001b[0m, in \u001b[0;36mDependency.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:565\u001b[0m, in \u001b[0;36mon.<locals>.wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(func):\n\u001b[0;32m--> 565\u001b[0m     \u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscroll_to_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscroll_to_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcancels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcurrency_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcurrency_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrigger_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrigger_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:605\u001b[0m, in \u001b[0;36mon\u001b[0;34m(triggers, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, every, js, concurrency_limit, concurrency_id, show_api)\u001b[0m\n\u001b[1;32m    599\u001b[0m     methods \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    600\u001b[0m         [EventListenerMethod(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    603\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     methods \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m         EventListenerMethod(t\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_trigger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, t\u001b[38;5;241m.\u001b[39mevent_name)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m triggers_typed\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    609\u001b[0m dep, dep_index \u001b[38;5;241m=\u001b[39m root_block\u001b[38;5;241m.\u001b[39mset_event_trigger(\n\u001b[1;32m    610\u001b[0m     methods,\n\u001b[1;32m    611\u001b[0m     fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     trigger_mode\u001b[38;5;241m=\u001b[39mtrigger_mode,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m set_cancel_events(methods, cancels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        duration_input = gr.Textbox(label=\"Duration in seconds\")\n",
    "        generated_audio = gr.Audio(label=\"Generated Audio\", autoplay=True)\n",
    "    \n",
    "    # duration_input.change(generate_tone1, inputs=duration_input, outputs=generated_audio)\n",
    "    @gr.on(triggers=generated_audio.stop, outputs=generated_audio)\n",
    "    def greet():\n",
    "        return pq.get()\n",
    "        # yield pipe5(\"\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RunnableLambda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_tone\u001b[39m(note):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m generate_tone2(note)\n\u001b[0;32m---> 12\u001b[0m runabl \u001b[38;5;241m=\u001b[39m \u001b[43mRunnableLambda\u001b[49m(func\u001b[38;5;241m=\u001b[39mgenerate_tone1,afunc\u001b[38;5;241m=\u001b[39mgenerate_tone)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# runabl.batch_as_completed()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RunnableLambda' is not defined"
     ]
    }
   ],
   "source": [
    "async def generate_tone2(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    yield await generate_tone2(note)\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone1,afunc=generate_tone)\n",
    "# runabl.batch_as_completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "\n",
    "batch_size = 3\n",
    "pq = Queue()\n",
    "\n",
    "def process_batch(batch):\n",
    "    results = []\n",
    "    for j in batch:\n",
    "        j = (j[0]+i, j[1])\n",
    "        results.append(j)\n",
    "    return results\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        futures.append(executor.submit(process_batch, batch))\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        for result in future.result():\n",
    "            pq.put_nowait(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 3\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i+batch_size]\n",
    "    for j in runabl.batch_as_completed(batch):\n",
    "        # print(j)\n",
    "        j=(j[0]+i,j[1])\n",
    "        pq.put_nowait(j)\n",
    "        # print(j)\n",
    "        # print(i+batch_size,batch)\n",
    "\n",
    "# remaining = len(chunks) % batch_size\n",
    "# if remaining > 0:\n",
    "#     batch = chunks[-remaining:]\n",
    "#     print(f\"Processing remaining elements: {batch}\")\n",
    "#     for j in runabl.batch_as_completed(batch):\n",
    "#         # print(j)\n",
    "#         j=(j[0]+i,j[1])\n",
    "#         pq.put_nowait(j)\n",
    "    # Do something with the remaining elements\n",
    "    # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "import queue\n",
    "from asyncio.queues import PriorityQueue\n",
    "import asyncio\n",
    "\n",
    "pq = PriorityQueue(30)\n",
    "chunks = c.split_text(text=text)\n",
    "chunks.remove(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nIn a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies',\n",
       " '. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another',\n",
       " '.\\n\\n    The two trucks, named Bessie and Betty, were old friends',\n",
       " '. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another',\n",
       " '. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious',\n",
       " '.\\n\\n    The two airplanes, named Falcon and Falconess, were equally impressive',\n",
       " '. They soared through the skies, their wings cutting through the clouds like knives',\n",
       " '. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places',\n",
       " '.\\n\\n    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys',\n",
       " '. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys',\n",
       " '.\\n\\n    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens',\n",
       " '. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations',\n",
       " '.\\n\\n    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming',\n",
       " '. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly',\n",
       " '. And they were proud to do it, day in and day out']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "QueueEmpty",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueueEmpty\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nowait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/queues.py:182\u001b[0m, in \u001b[0;36mQueue.get_nowait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove and return an item from the queue.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03mReturn an item if one is immediately available, else raise QueueEmpty.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mempty():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m QueueEmpty\n\u001b[1;32m    183\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wakeup_next(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_putters)\n",
      "\u001b[0;31mQueueEmpty\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pq.get_nowait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not pq.empty():\n",
    "    print(pq.get_nowait(),\"--\")\n",
    "    if pq.empty :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[38;5;28;01myield\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mget_nowait()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# get async function for precessing the text in batchhes of 3\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m         \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msepia\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstory\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mthen(fn\u001b[38;5;241m=\u001b[39mget2,inputs\u001b[38;5;241m=\u001b[39mstory ,outputs\u001b[38;5;241m=\u001b[39mnarrate)\u001b[38;5;241m.\u001b[39mthen(\u001b[38;5;28;01mlambda\u001b[39;00m _ : pq\u001b[38;5;241m.\u001b[39mclear())\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# gr.on()\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m():\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:605\u001b[0m, in \u001b[0;36mon\u001b[0;34m(triggers, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, every, js, concurrency_limit, concurrency_id, show_api)\u001b[0m\n\u001b[1;32m    599\u001b[0m     methods \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    600\u001b[0m         [EventListenerMethod(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    603\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     methods \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m         EventListenerMethod(t\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_trigger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, t\u001b[38;5;241m.\u001b[39mevent_name)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m triggers_typed\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    609\u001b[0m dep, dep_index \u001b[38;5;241m=\u001b[39m root_block\u001b[38;5;241m.\u001b[39mset_event_trigger(\n\u001b[1;32m    610\u001b[0m     methods,\n\u001b[1;32m    611\u001b[0m     fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     trigger_mode\u001b[38;5;241m=\u001b[39mtrigger_mode,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m set_cancel_events(methods, cancels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "import gradio as gr\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "import queue\n",
    "from asyncio.queues import PriorityQueue\n",
    "import asyncio\n",
    "\n",
    "pq = PriorityQueue(30)\n",
    "# chunks = c.split_text(text=text)\n",
    "# chunks.remove(\".\")\n",
    "# def generate_tone1(note):\n",
    "#     d = pipe5(note)\n",
    "#     return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        story = gr.TextArea(label=\"Duration in seconds\",show_copy_button=True)\n",
    "        image=gr.Image(type='pil')\n",
    "        narrate = gr.Audio(label=\"Generated Audio\", autoplay=True,streaming=True)\n",
    "    \n",
    "    # async def greet1():\n",
    "    #     d= await runabl.ainvoke(chunks[2])\n",
    "    #     print(type(d))\n",
    "    #     return  d\n",
    "\n",
    "        def sepia(input_img):\n",
    "            # chunks=[]\n",
    "            # async for e in chain.astream(input_img):\n",
    "            #     chunks.append(e)\n",
    "            #     yield \"\".join(chunks)\n",
    "            #             # chunks=[]\n",
    "            # for e in chain.astream(input_img):\n",
    "            #     chunks.append(e)\n",
    "            #     yield \"\".join(chunks)\n",
    "\n",
    "            return chain.ainvoke(input_img)\n",
    "        # chian\n",
    "\n",
    "        def get2(text):\n",
    "            batch_size = 3\n",
    "            # pq = PriorityQueue(30)\n",
    "            chunks = c.split_text(text=text)\n",
    "            chunks.remove(\".\")\n",
    "            for i in range(0, len(chunks), batch_size):\n",
    "                batch = chunks[i:i+batch_size]\n",
    "                # iterating the output from batch completion of langcahin.Runnable = runabl\n",
    "                for j in runabl.batch_as_completed(batch):\n",
    "                    # print(j)\n",
    "                    j=(j[0]+i,j[1])\n",
    "                    # appending the obejct in order\n",
    "                    pq.put_nowait(j)\n",
    "                for k in range(3):\n",
    "                    yield pq.get_nowait()[1]\n",
    "\n",
    "# get async function for precessing the text in batchhes of 3\n",
    "        gr.on(fn=sepia,triggers=image.upload,inputs=image,outputs=story).then(fn=get2,inputs=story ,outputs=narrate).then(lambda _ : pq.clear())\n",
    "\n",
    "    # gr.on()\n",
    "\n",
    "\n",
    "        async def get():\n",
    "            global chunks,runabl\n",
    "            batch_size = 3\n",
    "            for i in range(0, len(chunks), batch_size):\n",
    "                batch = chunks[i:i+batch_size]\n",
    "                # iterating the output from batch completion of langcahin.Runnable = runabl\n",
    "                for j in runabl.batch_as_completed(batch):\n",
    "                    # print(j)\n",
    "                    j=(j[0]+i,j[1])\n",
    "                    # appending the obejct in order\n",
    "                    pq.put_nowait(j)\n",
    "                    print(\"hello\")\n",
    "                yield\n",
    "            print(\"hello\")\n",
    "        \n",
    "        \n",
    "        async def get1():\n",
    "            get()\n",
    "    # def on_select(value, evt: gr.EventData):\n",
    "    #     return f\"The {evt.target} component was selected, and its value was {value}.\"\n",
    "    \n",
    "    # def bet():\n",
    "    #     i=0\n",
    "    #     if i == 0 :\n",
    "    #         time.sleep(4)\n",
    "    #     else:\n",
    "    #         return pq.get_nowait()[1]\n",
    "\n",
    "\n",
    "    # Do something with the batch, e.g. insert_posts(batch)\n",
    "\n",
    "# Handle any remaining elements\n",
    "    # remaining = len(records) % batch_size\n",
    "    # if remaining > 0:\n",
    "    #     batch = records[-remaining:]\n",
    "    #     print(f\"Processing remaining elements: {batch}\")\n",
    "        # Do something with the remaining elements\n",
    "\n",
    "# runabl.batch_as_completed\n",
    "\n",
    "    # gr.EventData\n",
    "    # gr.on(triggers=duration_input.change,)\n",
    "        # def next():\n",
    "        #     print(\"hello1\")\n",
    "        #     try:\n",
    "        #         f=pq.get_nowait()[1]\n",
    "        #     except asyncio.queues.QueueEmpty:\n",
    "        #         return None\n",
    "        #         # generated_audio.pause(lambda _ : 1)\n",
    "        #     return f\n",
    "\n",
    "    # duration_input.attach_load_event()\n",
    "    # duration_input.change(fn=get).then(fn=lambda _ : time.sleep(4),queue=False).fn() .then(fn=lambda _ : pq.get_nowait()[1],outputs=generated_audio)\n",
    "    # duration_input.change(fn=get).then(fn=next,outputs=generated_audio)\n",
    "    # generated_audio.(fn=lambda _ : pq.get_nowait(),outputs=generated_audio)\n",
    "    # generated_audio.stop(fn=next,outputs=generated_audio)\n",
    "\n",
    "    # duration_input.change(generate_tone1, inputs=duration_input, outputs=generated_audio)\n",
    "    # @gr.on(triggers=duration_input.change, outputs=generated_audio)\n",
    "\n",
    "        # yield pipe5(\"\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from PIL import Image\n",
    "\n",
    "pipe1 = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "pipe2 = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    temperature=0.5,\n",
    "    streaming=True\n",
    ")\n",
    "def reduce_add(a):\n",
    "    ll=dict()\n",
    "    for i in a:\n",
    "        if i['score'] > 0.89:\n",
    "            if i['label'] not in ll.keys():\n",
    "                ll[i['label']] = 1\n",
    "            else:\n",
    "                ll[i['label']]+=1\n",
    "    return \"there are \\n\"+', \\n'.join([str(i[1])+' '+i[0]  for i  in ll.items() ])\n",
    "\n",
    "def image_segmentation_tool(image: str):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe1(image)\n",
    "    if reduce_add(segmentation_results) == \"there are \\n\":\n",
    "        raise Passs()\n",
    "    return reduce_add(segmentation_results)\n",
    "\n",
    "def image_caption_tool(image: str):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe2(image)\n",
    "    if segmentation_results[0][\"generated_text\"] == \"\":\n",
    "        raise Passs(\"no result found use different image to create story\") \n",
    "    return segmentation_results[0][\"generated_text\"]\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def story_generation_tool(segmentation_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a storyteller. Based on the following segmentation results, create a story:\n",
    "    {segmentation_results}\n",
    "    Story:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    story = prompt | llm\n",
    "    return story.invoke(input={\"segmentation_results\":segmentation_results})\n",
    "\n",
    "# def translation_tool(english_text):\n",
    "#     prompt_template = \"\"\"\n",
    "#     You are a translator. Translate the following English text to Hindi:\n",
    "#     {english_text}\n",
    "\n",
    "#     Translation:\n",
    "#     \"\"\"\n",
    "#     prompt = PromptTemplate.from_template(prompt_template)\n",
    "#     translation = prompt | llm\n",
    "#     return translation.invoke(input={\"english_text\": english_text})\n",
    "\n",
    "\n",
    "runnable = RunnableLambda(image_segmentation_tool).with_fallbacks([RunnableLambda(image_caption_tool)])\n",
    "runnable2 = RunnableLambda(story_generation_tool)\n",
    "# runnable3 = RunnableLambda(translation_tool)\n",
    "\n",
    "chain = runnable | runnable2\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "title = \"Image to short Story Generator\"\n",
    "description = \"\"\"\n",
    "Upload an image, and this app will generate a short story based on the image.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sepia(input_img):\n",
    "    chunks=[]\n",
    "    for e in chain.astream(input_img):\n",
    "        chunks.append(e)\n",
    "        yield \"\".join(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Four friends, John, Mike, Sarah, and Tom, gathered in the park for their weekly pick-up soccer game. The sun was setting, casting long shadows over the grassy field. John, the team's goalkeeper, stood guard at the goalposts, while Mike and Tom took positions as midfielders. Sarah, the team's star striker, waited eagerly on the sidelines, ready to make her moves.\n",
      "\n",
      "As they kicked off, the game began in earnest. Mike and Tom passed the ball back and forth, weaving in and out of their opponents' defense. Sarah made a brilliant run towards the goal, but was tripped up just as she was about to shoot. The referee blew his whistle, awarding a free kick to their team.\n",
      "\n",
      "John stepped up, the ball at his feet. The opposing team's goalkeeper advanced, ready for the challenge. John took a deep breath, focusing on his aim. With a powerful kick, the ball soared through the air, arcing towards the goal. The goalkeeper leapt, but was unable to reach it in time. The ball bounced off the crossbar and into the net, sending the team into a frenzy of celebration.\n",
      "\n",
      "As they cheered and high-fived each other, John grinned, knowing that his team had once again emerged victorious. They played on into the twilight, the sound of their laughter and shouts carrying over the field, until the last rays of sunlight had faded away. And as they packed up their things and went their separate ways, they knew that they would be back again next week, ready to do it all over again.</s>\n"
     ]
    }
   ],
   "source": [
    "async for i in sepia(\"football-match.jpg\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gradio.components.image.Image'>\n",
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        story = gr.TextArea(label=\"Duration in seconds\",show_copy_button=True)\n",
    "        image=gr.Image(type='pil')\n",
    "        narrate = gr.Audio(label=\"Generated Audio\", autoplay=True,streaming=True)\n",
    "        print(type(image))\n",
    "        # gr.on(fn=sepia,triggers=image.upload,inputs=image,outputs=story)\n",
    "        # print(sepia(image))\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m image\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mImage(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m narrate \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mAudio(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoplay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msepia\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:605\u001b[0m, in \u001b[0;36mon\u001b[0;34m(triggers, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, every, js, concurrency_limit, concurrency_id, show_api)\u001b[0m\n\u001b[1;32m    599\u001b[0m     methods \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    600\u001b[0m         [EventListenerMethod(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    603\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     methods \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m         EventListenerMethod(t\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_trigger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, t\u001b[38;5;241m.\u001b[39mevent_name)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m triggers_typed\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    609\u001b[0m dep, dep_index \u001b[38;5;241m=\u001b[39m root_block\u001b[38;5;241m.\u001b[39mset_event_trigger(\n\u001b[1;32m    610\u001b[0m     methods,\n\u001b[1;32m    611\u001b[0m     fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     trigger_mode\u001b[38;5;241m=\u001b[39mtrigger_mode,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m set_cancel_events(methods, cancels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "        gr.on(fn=sepia,triggers=image.upload,inputs=image,outputs=story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Once upon a time in a small town nestled between rolling hills and a sparkling lake, there was a beautiful park where the community gathered to play and enjoy the sunshine. Four friends, Jack, Lily, Tom, and Sam, were some of the most dedicated park-goers. They spent their afternoons playing various games and sports, always encouraging each other to try new things and improve their skills.\n",
      "\n",
      "One sunny afternoon, as they were playing catch with a sports ball, Jack suggested they try something new. \"Why don't we start a friendly soccer match?\" he proposed. The others agreed, and they quickly set up goals using nearby trees and started the game.\n",
      "\n",
      "Jack, being the most experienced soccer player, took the position of goalkeeper. Lily, with her quick reflexes, became a formidable defender. Tom, known for his agility and speed, took the role of a midfielder. And Sam, the newest member of their group, was given the position of a forward.\n",
      "\n",
      "The game began, and despite Sam's initial struggles, the team worked together to help him improve. Jack made impressive saves, Lily blocked opposing players' shots, and Tom provided excellent passes. Sam, with renewed determination, started to score goals, and the team cheered him on.\n",
      "\n",
      "As the sun began to set, they packed up their things and headed home, feeling accomplished and happy. They had not only played a fun game of soccer but also supported and encouraged each other throughout. And they knew that the next time they gathered at the park, they would continue to explore new games and challenges together.</s>\n"
     ]
    }
   ],
   "source": [
    "async for e in chain.astream(\"football-match.jpg\"):\n",
    "        # chunks.append(e)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mget_nowait()[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msepia\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m chunks, runabl\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:605\u001b[0m, in \u001b[0;36mon\u001b[0;34m(triggers, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, every, js, concurrency_limit, concurrency_id, show_api)\u001b[0m\n\u001b[1;32m    599\u001b[0m     methods \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    600\u001b[0m         [EventListenerMethod(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    603\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     methods \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m         EventListenerMethod(t\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_trigger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, t\u001b[38;5;241m.\u001b[39mevent_name)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m triggers_typed\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    609\u001b[0m dep, dep_index \u001b[38;5;241m=\u001b[39m root_block\u001b[38;5;241m.\u001b[39mset_event_trigger(\n\u001b[1;32m    610\u001b[0m     methods,\n\u001b[1;32m    611\u001b[0m     fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     trigger_mode\u001b[38;5;241m=\u001b[39mtrigger_mode,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m set_cancel_events(methods, cancels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "import queue\n",
    "from asyncio.queues import PriorityQueue\n",
    "import asyncio\n",
    "\n",
    "pq = PriorityQueue(30)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        story = gr.TextArea(label=\"Duration in seconds\", show_copy_button=True)\n",
    "        image = gr.Image(type='pil')\n",
    "        narrate = gr.Audio(label=\"Generated Audio\", autoplay=True, streaming=True)\n",
    "\n",
    "    async def sepia(input_img):\n",
    "        chunks = []\n",
    "        # Assuming chain.astream(input_img) is a method call\n",
    "        async for e in chain.astream(input_img):\n",
    "            chunks.append(e)\n",
    "            yield \"\".join(chunks)\n",
    "\n",
    "    def get2(text):\n",
    "        batch_size = 3\n",
    "        chunks = c.split_text(text=text)\n",
    "        chunks = [chunk for chunk in chunks if chunk != \".\"]  # Remove periods from chunks\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i+batch_size]\n",
    "            # iterating the output from batch completion of langcahin.Runnable = runabl\n",
    "            for j in runabl.batch_as_completed(batch):\n",
    "                j = (j[0] + i, j[1])\n",
    "                pq.put_nowait(j)\n",
    "            for k in range(3):\n",
    "                yield pq.get_nowait()[1]\n",
    "\n",
    "    gr.on(fn=sepia, triggers=image.upload, inputs=image, outputs=story)\n",
    "\n",
    "    async def get():\n",
    "        global chunks, runabl\n",
    "        batch_size = 3\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i+batch_size]\n",
    "            for j in runabl.batch_as_completed(batch):\n",
    "                j = (j[0] + i, j[1])\n",
    "                pq.put_nowait(j)\n",
    "                print(\"hello\")\n",
    "            yield\n",
    "        print(\"hello\")\n",
    "\n",
    "    async def get1():\n",
    "        await get()\n",
    "\n",
    "    def next():\n",
    "        print(\"hello1\")\n",
    "        try:\n",
    "            f = pq.get_nowait()[1]\n",
    "        except asyncio.queues.QueueEmpty:\n",
    "            return None\n",
    "        return f\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "\n",
    "In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
    "\n",
    "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
    "\n",
    "    The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\n",
    "\n",
    "    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\n",
    "\n",
    "    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\n",
    "\n",
    "    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,\n",
       " array([ 6.4099895e-06, -3.3737240e-05, -2.8955947e-05, ...,\n",
       "         4.1919535e-05,  2.9555011e-05, -1.2921644e-05], dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await runabl.ainvoke(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "import queue\n",
    "from asyncio.queues import PriorityQueue\n",
    "\n",
    "pq = PriorityQueue()\n",
    "chunks = c.split_text(text=text)\n",
    "chunks.remove(\".\")\n",
    "\n",
    "# for i in range(0,len(chunks)):\n",
    "#     pq.put_nowait((i,await runabl.ainvoke(chunks[i])))\n",
    "# # chunks\n",
    "\n",
    "# while not pq.empty:\n",
    "#     print(pq.get_nowait(),\"--\")\n",
    "    # if pq.empty :\n",
    "    #     break\n",
    "# pq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py:1001: UserWarning: Expected 1 arguments for function <function handle_event at 0x7f27802e1a20>, received 0.\n",
      "  warnings.warn(\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py:1005: UserWarning: Expected at least 1 arguments for function <function handle_event at 0x7f27802e1a20>, received 0.\n",
      "  warnings.warn(\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py:1001: UserWarning: Expected 1 arguments for function <function <lambda> at 0x7f27801264d0>, received 0.\n",
      "  warnings.warn(\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py:1005: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x7f27801264d0>, received 0.\n",
      "  warnings.warn(\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py:1001: UserWarning: Expected 1 arguments for function <function stop_audio at 0x7f27802e31c0>, received 0.\n",
      "  warnings.warn(\n",
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py:1005: UserWarning: Expected at least 1 arguments for function <function stop_audio at 0x7f27802e31c0>, received 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/helpers.py:947: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 541, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1512, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 800, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_7231/1548060294.py\", line 30, in handle_event\n",
      "    await get()\n",
      "  File \"/tmp/ipykernel_7231/1548060294.py\", line 19, in get\n",
      "    for j in await runabl.batch_as_completed(batch):\n",
      "AttributeError: 'NoneType' object has no attribute 'batch_as_completed'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 541, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1928, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 833, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_7231/1548060294.py\", line 43, in <lambda>\n",
      "    duration_input.change(fn=handle_event).then(fn=lambda _: pq.get_nowait()[1], outputs=generated_audio)\n",
      "  File \"/usr/lib/python3.10/asyncio/queues.py\", line 182, in get_nowait\n",
      "    raise QueueEmpty\n",
      "asyncio.queues.QueueEmpty\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import asyncio\n",
    "import queue\n",
    "\n",
    "# Create a global queue for processed items\n",
    "pq = asyncio.Queue()\n",
    "\n",
    "# Initialize global variables\n",
    "chunks = []\n",
    "runabl = None  # This should be assigned to your Langchain.Runnable\n",
    "\n",
    "# Function to process the data in batches of 3\n",
    "async def get():\n",
    "    global chunks, runabl\n",
    "    batch_size = 3\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        # Iterating over the output from batch completion of langchain.Runnable = runabl\n",
    "        for j in await runabl.batch_as_completed(batch):\n",
    "            # Process and append the object in order\n",
    "            j = (j[0] + i, j[1])\n",
    "            await pq.put(j)\n",
    "\n",
    "# Function to handle the Gradio event\n",
    "async def handle_event(duration):\n",
    "    global chunks\n",
    "    # Here you would create the chunks based on the duration\n",
    "    # This is just a placeholder logic\n",
    "    chunks = [duration] * 9  # Example: creating 9 chunks from the duration\n",
    "    await get()\n",
    "    return await pq.get()\n",
    "\n",
    "# Function to handle stopping the audio\n",
    "async def stop_audio(_):\n",
    "    return await pq.get()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        duration_input = gr.Textbox(label=\"Duration in seconds\")\n",
    "        generated_audio = gr.Audio(label=\"Generated Audio\", autoplay=True, streaming=True)\n",
    "    \n",
    "    # Connecting the Gradio inputs and outputs\n",
    "    duration_input.change(fn=handle_event).then(fn=lambda _: pq.get_nowait()[1], outputs=generated_audio)\n",
    "    generated_audio.stop(fn=stop_audio, outputs=generated_audio)\n",
    "\n",
    "demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\n",
    "import asyncio\n",
    "from asyncio.queues import PriorityQueue\n",
    "\n",
    "# Create the text splitter\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "\n",
    "# Sample text (assuming you have a variable 'text' with the text you want to split)\n",
    "# text = \"Your text goes here. Make sure it is long enough to split.\"\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = c.split_text(text=text)\n",
    "\n",
    "# Remove empty chunks or specific characters if needed\n",
    "chunks = [chunk for chunk in chunks if chunk != \".\"]\n",
    "\n",
    "# Create a PriorityQueue\n",
    "pq = PriorityQueue()\n",
    "\n",
    "async def process_chunks(chunks):\n",
    "    # Process each chunk and put it into the PriorityQueue\n",
    "    async for i,v in runabl.abatch_as_completed(chunks):\n",
    "        await pq.put((i, v))\n",
    "        print(i, v)\n",
    "\n",
    "async def print_chunks():\n",
    "    # Print chunks from the PriorityQueue\n",
    "    while not pq.empty():\n",
    "        i, chunk = await pq.get()\n",
    "        print((i, chunk), \"--\")\n",
    "        pq.task_done()\n",
    "\n",
    "async def main():\n",
    "    # Run both tasks simultaneously\n",
    "    await asyncio.gather(process_chunks(chunks), print_chunks())\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # If there is no running event loop, use asyncio.run\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError:\n",
    "        # If an event loop is already running, create tasks and run them\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.create_task(main())\n",
    "        loop.run_until_complete(asyncio.gather(*asyncio.all_tasks(loop)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue.PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, (16000, array([-1.2501415e-05, -4.4408593e-05, -3.8727343e-05, ...,\n",
      "        3.9589697e-05,  2.7396680e-05, -2.0276231e-05], dtype=float32)))\n",
      "(4, (16000, array([ 7.2316743e-06, -3.1632011e-05, -2.7307138e-05, ...,\n",
      "        4.6607216e-05,  2.9522784e-05, -7.3716756e-06], dtype=float32)))\n",
      "(5, (16000, array([-6.6566921e-05, -1.6012025e-04, -8.3505765e-05, ...,\n",
      "        4.7119116e-05,  2.6652750e-05, -9.0038002e-06], dtype=float32)))\n",
      "(6, (16000, array([-3.4350692e-06, -3.7854159e-05, -3.1158266e-05, ...,\n",
      "        5.7494013e-05, -1.1056050e-05, -9.8604753e-05], dtype=float32)))\n",
      "(7, (16000, array([-1.9910735e-06, -3.3838252e-05, -2.7976983e-05, ...,\n",
      "        4.9033020e-05,  2.7951761e-05, -6.0080511e-06], dtype=float32)))\n",
      "(8, (16000, array([-5.1095799e-06, -3.5330693e-05, -3.0781102e-05, ...,\n",
      "        4.7544785e-05,  2.4969158e-05, -1.3209066e-05], dtype=float32)))\n",
      "(9, (16000, array([-6.8212921e-06, -3.7683589e-05, -3.0825493e-05, ...,\n",
      "        5.0159899e-05,  2.5722074e-05, -1.5758958e-05], dtype=float32)))\n",
      "(10, (16000, array([-3.8604767e-06, -3.4374691e-05, -3.0609575e-05, ...,\n",
      "        2.4314292e-04,  7.4048861e-05, -8.0675214e-05], dtype=float32)))\n",
      "(11, (16000, array([-5.6078665e-07, -3.6477511e-05, -3.1685606e-05, ...,\n",
      "       -1.0724028e-04, -3.3661537e-04, -5.3758826e-04], dtype=float32)))\n",
      "(12, (16000, array([-5.9988397e-06, -4.0362924e-05, -3.1633786e-05, ...,\n",
      "        5.2716619e-05,  2.6817541e-05, -9.0424264e-06], dtype=float32)))\n",
      "(13, (16000, array([-1.6504880e-06, -3.5386183e-05, -3.2288728e-05, ...,\n",
      "        5.6213856e-05,  3.3700962e-05, -2.0803955e-05], dtype=float32)))\n",
      "(14, (16000, array([-1.3844571e-03, -1.5564694e-03, -1.6473859e-03, ...,\n",
      "        4.2723790e-05,  3.2710668e-05, -1.7766697e-05], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "while not pq.empty():\n",
    "    print(pq.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 2: 3\n",
      "Result 4: 5\n",
      "Result 0: 1\n",
      "Result 1: 2\n",
      "Result 3: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable = RunnableLambda(lambda x: str(x))\n",
    "\n",
    "async def example():\n",
    "    async for index, result in runnable.abatch_as_completed([1, 2, 3, 4, 5]):\n",
    "        print(f\"Result {index}: {result}\")\n",
    "\n",
    "await example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priority: 1, Item: Item 2\n",
      "Priority: 2, Item: Item 3\n",
      "Priority: 3, Item: Item 1\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# Define a priority queue\n",
    "pq = queue.PriorityQueue()\n",
    "\n",
    "# Add items with a priority\n",
    "pq.put((3, \"Item 1\"))  # (priority, data)\n",
    "pq.put((1, \"Item 2\"))\n",
    "pq.put((2, \"Item 3\"))\n",
    "\n",
    "# Retrieve items in priority order\n",
    "while not pq.empty():\n",
    "    priority, item = pq.get()\n",
    "    print(f\"Priority: {priority}, Item: {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(producer_task, consumer_task)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kar911/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HUGGINGFACEHUB_API_TOKEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m pipe2 \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-to-text\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlpconnect/vit-gpt2-image-captioning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceEndpoint(\n\u001b[1;32m     24\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m     25\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     26\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m---> 27\u001b[0m     huggingfacehub_api_token\u001b[38;5;241m=\u001b[39m\u001b[43mHUGGINGFACEHUB_API_TOKEN\u001b[49m,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_add\u001b[39m(a):\n\u001b[1;32m     30\u001b[0m     ll\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HUGGINGFACEHUB_API_TOKEN' is not defined"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "title = \"Image to short Story Generator\"\n",
    "description = \"\"\"\n",
    "Upload an image, and this app will generate a short story based on the image.\n",
    "\"\"\"\n",
    "\n",
    "Full=[]\n",
    "chunks = []\n",
    "\n",
    "\n",
    "\n",
    "pipe1 = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\")\n",
    "pipe2 = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=100,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "def reduce_add(a):\n",
    "    ll=dict()\n",
    "    for i in a:\n",
    "        if i['score'] > 0.89:\n",
    "            if i['label'] not in ll.keys():\n",
    "                ll[i['label']] = 1\n",
    "            else:\n",
    "                ll[i['label']]+=1\n",
    "    return \"there are \\n\"+', \\n'.join([str(i[1])+' '+i[0]  for i  in ll.items() ])\n",
    "\n",
    "def image_segmentation_tool(image: str):\n",
    "    segmentation_results = pipe1(image)\n",
    "    if reduce_add(segmentation_results) == \"there are \\n\":\n",
    "        raise Passs()\n",
    "    return reduce_add(segmentation_results)\n",
    "\n",
    "def image_caption_tool(image: str):\n",
    "    # image = Image.open(image_path)\n",
    "    segmentation_results = pipe2(image)\n",
    "    if segmentation_results[0][\"generated_text\"] == \"\":\n",
    "        raise Passs(\"no result found use different image to create story\") \n",
    "    return segmentation_results[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "\n",
    "def story_generation_tool(segmentation_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a storyteller. Based on the following segmentation results, create a story:\n",
    "    {segmentation_results}\n",
    "\n",
    "    Story:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    story = prompt | llm\n",
    "    return story.invoke(input={\"segmentation_results\":segmentation_results})\n",
    "\n",
    "def translation_tool(english_text):\n",
    "    prompt_template = \"\"\"\n",
    "    You are a translator. Translate the following English text to Hindi:\n",
    "    {english_text}\n",
    "\n",
    "    Translation:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    translation = prompt | llm\n",
    "    return translation.invoke(input={\"english_text\": english_text})\n",
    "\n",
    "\n",
    "runnable = RunnableLambda(image_segmentation_tool).with_fallbacks([RunnableLambda(image_caption_tool)])\n",
    "runnable2 = RunnableLambda(story_generation_tool)\n",
    "runnable3 = RunnableLambda(translation_tool)\n",
    "\n",
    "chain = runnable | runnable2\n",
    "#-----\n",
    "\n",
    "# async def sepia(input_img):\n",
    "#     global chunks\n",
    "#     v = 0\n",
    "#     Full = []\n",
    "#     async for e in story.astream(input={\"segmentation_results\": input_img}):\n",
    "#         if v // 60 == 0:\n",
    "#             \"\".join(chunks)\n",
    "#             chunks.clear()\n",
    "#         chunks.append(e)\n",
    "#         Full.extend(chunks)\n",
    "#         v += 1\n",
    "#         yield \"\".join(Full)\n",
    "\n",
    "# async def sepia1(input_img):\n",
    "#     global chunks\n",
    "#     v = 0\n",
    "#     Full = []\n",
    "#     async for e in story.astream(input={\"segmentation_results\": input_img}):\n",
    "#         if v // 30 == 0:\n",
    "#             \"\".join(chunks)\n",
    "#             chunks.clear()\n",
    "#         chunks.append(e)\n",
    "#         Full.extend(chunks)\n",
    "#         v += 1\n",
    "#         yield \"\".join(Full)\n",
    "\n",
    "# async def combined_sepia_functions(input_img):\n",
    "#     sepia_gen = sepia(input_img)\n",
    "#     sepia1_gen = sepia1(input_img)\n",
    "    \n",
    "#     while True:\n",
    "#         try:\n",
    "#             result1 = await sepia_gen.__anext__()\n",
    "#         except StopAsyncIteration:\n",
    "#             result1 = None\n",
    "\n",
    "#         try:\n",
    "#             result2 = await sepia1_gen.__anext__()\n",
    "#         except StopAsyncIteration:\n",
    "#             result2 = None\n",
    "\n",
    "#         if result1 is None and result2 is None:\n",
    "#             break\n",
    "\n",
    "#         yield (result1, result2)\n",
    "\n",
    "async def sepia(input_img):\n",
    "    v=0\n",
    "    async for e in story.astream(input={\"segmentation_results\":input_img}):\n",
    "        if v//60== 0:\n",
    "            \"\".join(chunks)\n",
    "            chunks.clear()\n",
    "        chunks.append(e)\n",
    "        Full.extend(chunks)\n",
    "        v=+1\n",
    "        yield \"\".join(Full)\n",
    "\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(type='pil'), \"textarea\",title=title,\n",
    "    description=description,live=True\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "\n",
    "#-----\n",
    "async def generate_tone2(note):\n",
    "    if note[0] > 2:\n",
    "        await asyncio.sleep(5)\n",
    "    d = pipe5(note[1])\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    yield await generate_tone2(note)\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone1)\n",
    "#-----\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "import queue\n",
    "\n",
    "pq = queue.PriorityQueue()\n",
    "chunks = c.split_text(text=text)\n",
    "chunks.remove(\".\")\n",
    "async for i,v in runabl.abatch_as_completed(enumerate(chunks)):\n",
    "    pq.put((i, v))\n",
    "\n",
    "#-----\n",
    "import gradio as gr\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        duration_input = gr.Textbox(label=\"Duration in seconds\")\n",
    "        generated_audio = gr.Audio(label=\"Generated Audio\", autoplay=True)\n",
    "    \n",
    "    duration_input.change(generate_tone1, inputs=duration_input, outputs=generated_audio)\n",
    "    @gr.on(triggers=[generated_audio.stop], outputs=generated_audio)\n",
    "    def greet():\n",
    "        return pq.get()\n",
    "    \n",
    "\n",
    "        # yield pipe5(\"\")\n",
    "\n",
    "#count of RecursiveCharacterTextSplitter splits\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "\n",
    "\n",
    "async def generate_tone2(note):\n",
    "    if note[0] > 2:\n",
    "        await asyncio.sleep(1)\n",
    "    d = pipe5(note[1])\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone1(note):\n",
    "    if note[0] > 2:\n",
    "        await asyncio.sleep(1)\n",
    "    d = pipe5(note[1])\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    yield await generate_tone2(note)\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone)\n",
    "# runabl = RunnableLambda(func=generate_tone1)\n",
    "#-----\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "import queue\n",
    "\n",
    "pq = queue.PriorityQueue()\n",
    "\n",
    "\n",
    "#-----\n",
    "import gradio as gr\n",
    "\n",
    "def generate_tone1(note):\n",
    "    d = pipe5(note)\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def data(chunks):\n",
    "    async for i,v in runabl.abatch_as_completed(enumerate(chunks)):\n",
    "        pq.put((i, v))\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        duration_input = gr.Textbox(label=\"Duration in seconds\")\n",
    "        generated_audio = gr.Audio(label=\"Generated Audio\", autoplay=True,streaming=True)\n",
    "    \n",
    "    # duration_input.change(generate_tone1, inputs=duration_input, outputs=generated_audio)\n",
    "    \n",
    "    @gr.on(triggers=[duration_input.change],inputs=duration_input)\n",
    "    async def greet2(text):\n",
    "        chunks = c.split_text(text=text)\n",
    "\n",
    "        if \".\" in chunks:\n",
    "            chunks.remove(\".\")\n",
    "        data(chunks)\n",
    "\n",
    "    # @gr.on(triggers=[duration_input.change],inputs=duration_input)\n",
    "    # def greet1(text): \n",
    "\n",
    "    @gr.on(triggers=[generated_audio.stop], outputs=generated_audio)\n",
    "    def greet():\n",
    "        return pq.get()\n",
    "    \n",
    "\n",
    "        # yield pipe5(\"\")\n",
    "\n",
    "#count of RecursiveCharacterTextSplitter splits\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     duration_input \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuration in seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     generated_audio \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mAudio(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoplay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;129;43m@gr\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 45\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mgreet2\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:97\u001b[0m, in \u001b[0;36mDependency.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:565\u001b[0m, in \u001b[0;36mon.<locals>.wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(func):\n\u001b[0;32m--> 565\u001b[0m     \u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtriggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscroll_to_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscroll_to_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcancels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcurrency_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcurrency_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_api\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrigger_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrigger_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/DataScience_LLM/.venv/lib/python3.10/site-packages/gradio/events.py:605\u001b[0m, in \u001b[0;36mon\u001b[0;34m(triggers, fn, inputs, outputs, api_name, scroll_to_output, show_progress, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, every, js, concurrency_limit, concurrency_id, show_api)\u001b[0m\n\u001b[1;32m    599\u001b[0m     methods \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    600\u001b[0m         [EventListenerMethod(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchange\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    603\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     methods \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m         EventListenerMethod(t\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mhas_trigger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, t\u001b[38;5;241m.\u001b[39mevent_name)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m triggers_typed\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    609\u001b[0m dep, dep_index \u001b[38;5;241m=\u001b[39m root_block\u001b[38;5;241m.\u001b[39mset_event_trigger(\n\u001b[1;32m    610\u001b[0m     methods,\n\u001b[1;32m    611\u001b[0m     fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m     trigger_mode\u001b[38;5;241m=\u001b[39mtrigger_mode,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m set_cancel_events(methods, cancels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import time\n",
    "import asyncio\n",
    "import queue\n",
    "\n",
    "# Assuming pipe5 is defined elsewhere in your code\n",
    "# pipe5 = pipeline(\"text-to-speech\")  # or whatever pipeline you are using\n",
    "\n",
    "async def generate_tone2(note):\n",
    "    if note[0] > 2:\n",
    "        await asyncio.sleep(1)\n",
    "    d = pipe5(note[1])\n",
    "    return (d[\"sampling_rate\"], d[\"audio\"].squeeze())\n",
    "\n",
    "async def generate_tone(note):\n",
    "    yield await generate_tone2(note)\n",
    "\n",
    "runabl = RunnableLambda(func=generate_tone)\n",
    "c = RecursiveCharacterTextSplitter(separators=[\".\"], chunk_size=20, chunk_overlap=0)\n",
    "pq = queue.PriorityQueue()\n",
    "\n",
    "async def data(chunks):\n",
    "    async for i, v in runabl.abatch_as_completed(enumerate(chunks)):\n",
    "        pq.put((i, v))\n",
    "\n",
    "def get_audio_from_queue():\n",
    "    while True:\n",
    "        if not pq.empty():\n",
    "            i, v = pq.get()\n",
    "            yield v[1], v[0]  # Return audio data and sample rate\n",
    "        else:\n",
    "            time.sleep(0.1)  # Wait for more data\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        duration_input = gr.Textbox(label=\"Duration in seconds\")\n",
    "        generated_audio = gr.Audio(label=\"Generated Audio\", autoplay=True, streaming=True)\n",
    "\n",
    "    @gr.on(inputs=duration_input, triggers=duration_input.change)\n",
    "    def greet2(text):\n",
    "        chunks = c.split_text(text=text)\n",
    "        if \".\" in chunks:\n",
    "            chunks.remove(\".\")\n",
    "        data(chunks)\n",
    "\n",
    "    @gr.on(outputs=generated_audio, triggers=generated_audio.stream)\n",
    "    def stream_audio():\n",
    "        return get_audio_from_queue()\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "\n",
    "In a bustling world filled with the constant hum of transportation, two trucks and two airplanes ruled the roads and skies. These mighty vehicles were the lifeblood of the economy, carrying goods and people from one place to another.\n",
    "\n",
    "    The two trucks, named Bessie and Betty, were old friends. They had been on the road together for years, their engines purring like contented cats as they carried their precious cargo from one destination to another. Bessie was a sturdy flatbed truck, her bed filled with crates and barrels, while Betty was a sleek refrigerated truck, her cold heart keeping perishable goods fresh and delicious.\n",
    "\n",
    "    The two airplanes, named Falcon and Falconess, were equally impressive. They soared through the skies, their wings cutting through the clouds like knives. Falcon was a cargo plane, his hold filled with pallets and containers, while Falconess was a passenger plane, her cabin filled with excited travelers, eager to explore new places.\n",
    "\n",
    "    Every day, Bessie and Betty would make their way along the highways, their engines rumbling as they climbed hills and descended into valleys. They would pass each other on the road, exchanging friendly honks and waves, before continuing on their separate journeys.\n",
    "\n",
    "    Meanwhile, Falcon and Falconess would take to the skies, their powerful engines roaring as they ascended into the heavens. They would fly in formation, their wings glinting in the sun as they cruised above the world, before parting ways to deliver their cargo to their respective destinations.\n",
    "\n",
    "    And so, the two trucks and two airplanes continued their daily routine, their engines never faltering, their spirits never dimming. They were the unsung heroes of the transportation world, the unseen workers keeping the economy running smoothly. And they were proud to do it, day in and day out.\n",
    "\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
